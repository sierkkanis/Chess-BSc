import numpy as np
import random
import theano
import theano.tensor as T
import lasagne
from lasagne.layers import InputLayer, DenseLayer
from lasagne.nonlinearities import LeakyRectify, sigmoid, rectify
import matplotlib.pyplot as plt
import matplotlib
import cPickle as pickle
import os
import time
from PIL import Image
import PIL
import sys
import scipy.misc
import xboard
import sunfish
import chess
import gaviota

####################
# Chess implementation
###################

board = 0
board2D = 0
pieceDict = {'K' : 0, 'k' : 1, 'P' : 2}
pieceDict2 = {0 : 'K', 1 : 'k', 2: 'P'}
side = True
halfmoves = 0
reward = 0
discount = 0.9
piecesList = []
KCoors = (0,0)
kCoors = (0,0)
PCoors = (0,0)
boardSize = 8
saveNumber = 0
maxNodesEngine = 200
draw50Moves = 0
randomMoves = 0.0

boardSunFish = list(
	'         \n'  # 0 -  9
	'         \n'  # 10 - 19
	' ........\n'  # 20 - 29
	' ........\n'  # 30 - 39
	' ........\n'  # 40 - 49
	' ........\n'  # 50 - 59
	' ........\n'  # 60 - 69
	' ........\n'  # 70 - 79
	' ........\n'  # 80 - 89
	' ........\n'  # 90 - 99
	'         \n'  # 100 -109
	'          '  # 110 -119
)

# for board representation
noPieceNumber = -1.0/(boardSize*boardSize)
PieceNumber = ((boardSize*boardSize) - 1.0)/(boardSize*boardSize)
# prints the coors of all pieces on a 2d board
def boardTo2D():
	global board2D
	global board
	global piecesList
	global boardSize
	board2D = np.chararray((boardSize,boardSize))
	board2D[:] = '.'
	print piecesList
	for piece in piecesList:
		x, y = getCoor(piece)
		board2D[y][x] = piece

# nice print
def printBoard():
	global board2D
	global side
	boardTo2D()
	print board2D
	print "side = ", side, ", legal = ", isLegal(), ", terminal = ", isTerminalState()

# to make custom size chessboard
# print new.shape
#
# for x in range(0,800):
# 	for y in range(0,800):
# 		for z in range(0, 3):
# 			# print new[x][x]
# 			new[x][y][z] = 0
#
# for i in range(4):
# 	for j in range(4):
# 		p = 0
# 		if i%2 == 0:
# 			p = 1
# 		for y in range(2*i-p*100,(2*i+1-p)*100):
# 			for x in range((2*j*100),((2*j+1)*100)):
# 				for z in range(0,3):
# 					#print new[x][x]
# 					new[x][y][z] = 255
# 					#print new[x][x]

def boardToImage():
	global saveNumber
	global piecesList
	newimage = Image.open("Images/chessboard.jpg")
	newimage = newimage.resize((800, 800), PIL.Image.ANTIALIAS)
	board = np.asarray(newimage)
	board.flags.writeable = True

	bk = Image.open("Images/blackking.jpg")
	bk = bk.resize((100, 100), PIL.Image.ANTIALIAS)
	bk = bk.convert("L")
	bk = np.asarray(bk)

	wk = Image.open("Images/whiteking.jpg")
	wk = wk.resize((100, 100), PIL.Image.ANTIALIAS)
	wk = wk.convert("L")
	wk = np.asarray(wk)

	p = Image.open("Images/blackpawn.png")
	p = p.resize((100, 100), PIL.Image.ANTIALIAS)
	p = p.convert("L")
	p = np.asarray(p)

	x, y = getCoor('K')
	board = placePieceImage(bk, board, x, y)
	x, y = getCoor('k')
	board = placePieceImage(wk, board, x, y)
	if 'P' in piecesList:
		x, y = getCoor('P')
		board = placePieceImage(p, board, x, y)

	filename = "Games/board" + str(saveNumber) + ".jpg"
	scipy.misc.imsave(filename, board)
	saveNumber +=1

def placePieceImage(piece, board, xC, yC):
	xC = xC *100
	yC = yC * 100

	for x in range(0,100):
		for y in range(0,100):
			for z in range(0,3):
				board[x+yC][y+xC][z] = piece[x][y]
	return board

# reset all board parameters
def boardInit(amountPieces):
	global boardSize
	global board
	global noPieceNumber
	global piecesList
	global KCoors
	global kCoors
	global PCoors
	global side
	global boardSunFish
	global draw50Moves

	side = True
	KCoors = (0, 0)
	kCoors = (0, 0)
	PCoors = (0, 0)
	piecesList = []
	draw50Moves = 0
	boardSunFish = list(
		'         \n'  # 0 -  9
		'         \n'  # 10 - 19
		' ........\n'  # 20 - 29
		' ........\n'  # 30 - 39
		' ........\n'  # 40 - 49
		' ........\n'  # 50 - 59
		' ........\n'  # 60 - 69
		' ........\n'  # 70 - 79
		' ........\n'  # 80 - 89
		' ........\n'  # 90 - 99
		'         \n'  # 100 -109
		'          '  # 110 -119
	)
	board = np.empty(boardSize*boardSize*amountPieces).reshape((amountPieces,boardSize,boardSize))
	for x in range(len(board)):
		for y in range(len(board[x])):
			board[x][y] = noPieceNumber

def switchSide():
	global side
	if side == True:
		side = False
	else:
		side = True

def getCoor(piece):
	global KCoors
	global kCoors
	global PCoors
	if piece == 'K':
		x, y = KCoors
		return x,y
	if piece == 'k':
		x, y = kCoors
		return x,y
	if piece == 'P':
		x, y = PCoors
		return x,y

# switches side when the coors are changed.
def setCoor(piece, x, y):
	global KCoors
	global kCoors
	global PCoors
	if piece == 'K':
		KCoors = (x, y)
	if piece == 'k':
		kCoors = (x, y)
	if piece == 'P':
		PCoors = (x, y)
	switchSide()

# checks whether square is free
def freeSquare(x, y):
	global piecesList
	for piece in piecesList:
		xx, yy = getCoor(piece)
		if x == xx and y == yy:
			return False
	return True

# set piece, doesn't remove the old, absolute coordinates
def setPiece(piece, x, y):
	global board
	global pieceDict
	global PieceNumber
	global piecesList
	if freeSquare(x, y):
		board[pieceDict.get(piece)][y][x] = PieceNumber
		setCoor(piece, x, y)
		piecesList.append(piece)
		return True
	else:
		return False

def setPieceRandom(piece):
	onFreeSquare = False
	global boardSize
	# checks whether square is free to add piece.
	while onFreeSquare == False:
		xRandom = random.randint(0,boardSize-1)
		yRandom = random.randint(0,boardSize-1)
		onFreeSquare = setPiece(piece, xRandom, yRandom)

# random with given boundaries
def setPieceSemiRandom(piece, x1, x2, y1, y2):
	onFreeSquare = False
	# if the coordinates given are out of board
	if x1 < 0:
		x1 = 0
	if x2 > 7:
		x2 = 7
	# checks whether square is free to add piece.
	while onFreeSquare == False:

		xRandom = random.randint(x1,x2)
		yRandom = random.randint(y1,y2)
		onFreeSquare = setPiece(piece, xRandom, yRandom)

# translates my board representation to that over engine SunFish
def boardToSunFish():
	global boardSunFish
	global piecesList
	boardSunFish = list(
		'         \n'  # 0 -  9
		'         \n'  # 10 - 19
		' ........\n'  # 20 - 29
		' ........\n'  # 30 - 39
		' ........\n'  # 40 - 49
		' ........\n'  # 50 - 59
		' ........\n'  # 60 - 69
		' ........\n'  # 70 - 79
		' ........\n'  # 80 - 89
		' ........\n'  # 90 - 99
		'         \n'  # 100 -109
		'          '  # 110 -119
	)
	for piece in piecesList:
		x, y = getCoor(piece)
		place = 21 + x + y*10
		boardSunFish[place] = piece
	return "".join(boardSunFish)

##############
# board setups
##############

def boardSetupStalemate():
	global side
	boardInit(3)
	setPiece('P', 0, 1)
	setPiece('k', 2, 0)
	setPiece('K', 0, 0)
	side = True

def boardSetupPawn():
	global side
	boardInit(3)
	setPieceSemiRandom('P', 0,7,1,6)
	# setPiece('P', 4,0)
	# setPiece('K',4,4)
	setPieceRandom('K')
	setPieceRandom('k')
	if isLegal() == False:
		boardSetupPawn()
	side = True

def boardSetupPawnEasy():
	global side
	boardInit(3)
	setPieceSemiRandom('P', 0,7,1,6)
	xP, yP = getCoor('P')

	setPieceRandom('K')
	# always further down than pawn, so can't reach it.
	setPieceSemiRandom('k', 0, 7, yP+1, 7)
	if isLegal() == False:
		boardSetupPawn()
	side = True

def boardSetupPawnHard():
	global side
	boardInit(3)
	setPieceSemiRandom('P', 0,7,3,6)
	xP, yP = getCoor('P')

	setPieceSemiRandom('K', xP -2, xP +2, yP , yP +1)
	setPieceSemiRandom('k', xP-2, xP+2, 0, yP)
	if isLegal() == False:
		boardSetupPawn()
	side = True

def boardSetupPawnMedium():
	global side
	boardInit(3)
	setPieceSemiRandom('P', 0,7,3,6)
	xP, yP = getCoor('P')

	setPieceSemiRandom('K', xP -2, xP +2, yP -2 , yP -1)
	setPieceSemiRandom('k', xP-2, xP+2, yP -1, yP)
	if isLegal() == False:
		boardSetupPawn()
	side = True

def boardSetupPawn2():
	global side
	boardInit(3)
	setPiece('P', 4, 1)
	setPiece('k', 7, 2)
	setPiece('K', 6, 0)
	side = True

# returns -1, 0 or 1 corresponding to whether a position is won or not.
def boardEvaluation():
	board = chess.Board(None)

	piece = chess.Piece(chess.PAWN,chess.WHITE)
	x, y = getCoor('P')
	place = x + (7-y)*8
	board.set_piece_at(place,piece)

	piece = chess.Piece(chess.KING,chess.BLACK)
	x, y = getCoor('k')
	place = x + (7-y)*8
	board.set_piece_at(place,piece)

	piece = chess.Piece(chess.KING,chess.WHITE)
	x, y = getCoor('K')
	place = x + (7-y)*8
	board.set_piece_at(place,piece)

	with gaviota.open_tablebases("data/gaviota") as tablebases:
		return tablebases.probe_wdl(board)

#################
# move the pieces
#################

# moves king into direction
def movePiece(direction, piece):
	global board
	global reward
	global PieceNumber
	global noPieceNumber
	global pieceDict
	x, y = getCoor(piece)
	if direction == 0:
		board[pieceDict.get(piece)][y-1][x] = PieceNumber
		board[pieceDict.get(piece)][y][x] = noPieceNumber
		setCoor(piece, x, y-1)
	if direction == 1:
		board[pieceDict.get(piece)][y-1][x-1] = PieceNumber
		board[pieceDict.get(piece)][y][x] = noPieceNumber
		setCoor(piece, x-1, y-1)
	if direction == 2:
		board[pieceDict.get(piece)][y-1][x+1] = PieceNumber
		board[pieceDict.get(piece)][y][x] = noPieceNumber
		setCoor(piece, x+1, y-1)
	if direction == 3:
		board[pieceDict.get(piece)][y][x-1] = PieceNumber
		board[pieceDict.get(piece)][y][x] = noPieceNumber
		setCoor(piece, x-1, y)
	if direction == 4:
		board[pieceDict.get(piece)][y][x+1] = PieceNumber
		board[pieceDict.get(piece)][y][x] = noPieceNumber
		setCoor(piece, x+1, y)
	if direction == 5:
		board[pieceDict.get(piece)][y+1][x] = PieceNumber
		board[pieceDict.get(piece)][y][x] = noPieceNumber
		setCoor(piece, x, y+1)
	if direction == 6:
		board[pieceDict.get(piece)][y+1][x-1] = PieceNumber
		board[pieceDict.get(piece)][y][x] = noPieceNumber
		setCoor(piece, x-1, y+1)
	if direction == 7:
		board[pieceDict.get(piece)][y+1][x+1] = PieceNumber
		board[pieceDict.get(piece)][y][x] = noPieceNumber
		setCoor(piece, x+1, y+1)
	if direction == 8:
		board[pieceDict.get(piece)][y - 1][x] = PieceNumber
		board[pieceDict.get(piece)][y][x] = noPieceNumber
		setCoor(piece, x, y - 1)

def checkIsLegal(piece, x,y):
	if isLegal():
		setCoor(piece, x, y)
		return True
	else:
		setCoor(piece, x, y)
		return False

# black has no other pieces
def ownPieceOnCoor(king, x, y, selfPiece):
	global piecesList
	if king == 'K':
		for piece in piecesList:
			xx, yy = getCoor(piece)
			if x == xx and y == yy and piece != selfPiece:
				return True
		return False
	if king == 'k':
		return False

# checks for illegal and off board moves
def tryToMoveKing(direction, king):
	global board
	global reward
	global PieceNumber
	global noPieceNumber
	global pieceDict
	global boardSize
	x, y = getCoor(king)
	if direction == 0 and y != 0 and ownPieceOnCoor(king, x, y-1, king) == False:
		setCoor(king, x, y-1)
		return checkIsLegal(king, x,y)
	if direction == 1 and y != 0 and x != 0 and ownPieceOnCoor(king, x-1, y-1, king) == False:
		setCoor(king, x-1, y-1)
		return checkIsLegal(king, x, y)
	if direction == 2 and y != 0 and x != boardSize-1 and ownPieceOnCoor(king, x+1, y-1, king) == False:
		setCoor(king, x+1, y-1)
		return checkIsLegal(king, x, y)
	if direction == 3 and x != 0 and ownPieceOnCoor(king, x-1, y, king) == False:
		setCoor(king, x-1, y)
		return checkIsLegal(king, x, y)
	if direction == 4 and x != boardSize-1 and ownPieceOnCoor(king, x+1, y, king) == False:
		setCoor(king, x+1, y)
		return checkIsLegal(king, x, y)
	if direction == 5 and y != boardSize-1 and ownPieceOnCoor(king, x, y+1, king) == False:
		setCoor(king, x, y+1)
		return checkIsLegal(king, x, y)
	if direction == 6 and y != boardSize-1 and x != 0 and ownPieceOnCoor(king, x-1, y+1, king) == False:
		setCoor(king, x-1, y+1)
		return checkIsLegal(king, x, y)
	if direction == 7 and y != boardSize-1 and x != boardSize-1 and ownPieceOnCoor(king, x+1, y+1, king) == False:
		setCoor(king, x+1, y+1)
		return checkIsLegal(king, x, y)
	else:
		return False

#################
# move white pieces
#################

# set coors but also back
def tryToMovePawn(direction):
	global board
	global piecesList
	if 'P' in piecesList:
		x, y = getCoor('P')
		if direction == 8 and y != 0 and ownPieceOnCoor('K', x, y-1, 'P') == False:
			setCoor('P', x, y - 1)
			return checkIsLegal('P', x,y)
		else:
			return False
	else:
		return False

def tryToMovePiece(direction):
	if direction <= 7:
		return tryToMoveKing(direction, 'K'), 'K'
	if direction == 8:
		return tryToMovePawn(direction), 'P'

def movePieceIfPossible(direction):
	moveIsPossible, piece = tryToMovePiece(direction)
	if moveIsPossible == None:
		print 'NONE!'
		print piece
	if moveIsPossible == True:
		movePiece(direction, piece)
		return True
	else:
		return False


#############
# move black king
#############

def moveBlackKingRandom():
	legalMove = False
	while legalMove == False and stalemate() == False:
		direction = random.randint(0, 7)
		legalMove = tryToMoveKing(direction, 'k')
		if legalMove == True:
			movePiece(direction, 'k')
			takePieceWhenPossible()

# when black king stands on top of pawn
def takePieceWhenPossible():
	global noPieceNumber
	global piecesList
	if 'P' in piecesList:
		xk, yk = getCoor('k')
		xP, yP = getCoor('P')
		if xk == xP and yk == yP:
			board[2][yP][xP] = noPieceNumber
			piecesList.remove('P')

# pos = xboard.parseFEN('8/8/8/1k6/8/2P5/8/K7 w - - 0 1')
# print pos
#
# boardSetupPawn2()
# boardSun = boardToSunFish()
# print boardSun
# # do the other params make sense?
# score = sum(sunfish.pst[p][i] for i, p in enumerate(boardSun) if p.isupper())
# score -= sum(sunfish.pst[p.upper()][i] for i, p in enumerate(boardSun) if p.islower())
# print score

def moveBlackKing(maxn):
	global noPieceNumber
	global PieceNumber
	global board
	global draw50Moves
	global randomMoves
	# transforms board to sunfish representation
	boardSun = boardToSunFish()
	# do the other params make sense?
	score = sum(sunfish.pst[p][i] for i, p in enumerate(boardSun) if p.isupper())
	score -= sum(sunfish.pst[p.upper()][i] for i, p in enumerate(boardSun) if p.islower())
	pos = sunfish.Position(boardSun, score=score, wc=(False, False), bc=(False, False), ep=0, kp=0)
	# to make black move
	pos = pos.rotate()
	if stalemate() == False:
		m, _ = sunfish.search(pos, maxn)
		if m != None:
			move = xboard.mrender(0, pos, m)
			# to change back to my representation
			sunfishMoveDict = {'a': 7, 'b': 6, 'c': 5, 'd': 4, 'e': 3, 'f': 2, 'g': 1, 'h': 0}
			y = int(move[3]) - 1
			x = sunfishMoveDict.get(move[2])
			# performs the new move
			xOld, yOld = getCoor('k')
			setCoor('k', x, y)
			draw50Moves += 1
			if isLegal() == False:
				setCoor('k', xOld, yOld)
				moveBlackKingRandom()
				randomMoves += 1
				print 'Black did a random move'
			else:
				board[1][yOld][xOld] = noPieceNumber
				board[1][y][x] = PieceNumber
				takePieceWhenPossible()
		else:
			moveBlackKingRandom()
			print 'SUNFISH MOVE WAS NONE'

#################
# board position checks
#################

# for black king and one pawn
def inCheck():
	global board
	global piecesList
	if 'P' in piecesList:
		xP, yP = getCoor('P')
		xk, yk = getCoor('k')
		if yk == yP -1 and (xk == xP - 1 or xk == xP + 1):
			return True
	else:
		return False

# when kings touch or are in check without having the move
def isLegal():
	global side
	global piecesList
	if side == True and inCheck():
		return False
	# only when more pieces are on board
	if len(piecesList) > 1:
		xk, yk = getCoor('k')
		xK, yK = getCoor('K')
		if ((yk == yK -1 and (xk == xK - 1 or xk == xK or xk == xK + 1))
			or (yk == yK and (xk == xK - 1 or xk == xK or xk == xK + 1))
			or (yk == yK +1 and (xk == xK - 1 or xk == xK or xk == xK + 1))
			or (xk == xK and yk == yK)):
			return False
		else:
			return True
	else:
		return True

# only checked after both kings moved
def isTerminalState():
	global reward
	global piecesList
	global draw50Moves
	if 'P' in piecesList:
		xP, yP = getCoor('P')
		if yP == 0:
			return 'won'
		elif stalemate():
			return 'drawn'
		# elif draw50Moves > 50:
		# 	print '50 move rule'
		# 	return 'drawn'
		else:
			return False
	else:
		return 'drawn'

# stalemate
def stalemate():
	global side
	kCanMove = False
	KCanMove = False
	for i in range(7):
		if tryToMoveKing(i,'k') == True:
			kCanMove = True
	for i in range(7):
		if tryToMoveKing(i, 'K') == True:
			KCanMove = True
	if tryToMovePawn(8) == True:
		KCanMove = True

	if kCanMove == False and side == False:
		return True
	elif KCanMove == False and side == True:
		return True
	else:
		return False

#######################
# DEEP REINFORCEMENT LEARNING
#######################
batchSize = 32
learningRate = 0.001

boardSetupPawn()
# print board
# print np.ones(192)
# print np.array([np.ones(192)])
# shapeBoardOneBatch = np.expand_dims(board.flatten(), 0).shape
shapeBoard = len(board.flatten())


X_sym = T.matrix()
Y_sym = T.matrix()
rew = T.ivector()
disc = T.vector()
action = T.ivector()
action_target = T.ivector()

# linear regression (?)
l_in = lasagne.layers.InputLayer(shape=(None, shapeBoard))
l_hidden = lasagne.layers.DenseLayer(l_in, num_units=100, nonlinearity = rectify)
l_hidden2 = lasagne.layers.DenseLayer(l_hidden, num_units=50, nonlinearity = rectify)
l_hidden3 = lasagne.layers.DenseLayer(l_hidden2, num_units=25, nonlinearity = rectify)
l_out = lasagne.layers.DenseLayer(l_hidden3, num_units=9, nonlinearity = None)
all_param_values = lasagne.layers.get_all_param_values(l_out)

# same but for target, copy weights from trainable network
l_in_target = lasagne.layers.InputLayer(shape=(None, shapeBoard))
l_hidden_target = lasagne.layers.DenseLayer(l_in_target, num_units=100, nonlinearity = rectify)
l_hidden2_target = lasagne.layers.DenseLayer(l_hidden_target, num_units=50, nonlinearity = rectify)
l_hidden3_target = lasagne.layers.DenseLayer(l_hidden2_target, num_units=25, nonlinearity = rectify)
l_out_target = lasagne.layers.DenseLayer(l_hidden3_target, num_units=9, nonlinearity = None)
lasagne.layers.set_all_param_values(l_out_target, all_param_values)

# trainable network
output = lasagne.layers.get_output(l_out, X_sym)
params = lasagne.layers.get_all_params(l_out)
actionBest = output.argmax(-1)
actionValueBest = output.max(-1)
actionValue = output[0][action]
actionValues = output[0]

# target network
output_target = lasagne.layers.get_output(l_out_target, Y_sym)
params_target = lasagne.layers.get_all_params(l_out_target)
actionBestTargetValue = output_target.max()
actionBestTarget = output_target.argmax(-1)

# general lasagne
#loss = lasagne.objectives.squared_error(disc*actionBestTargetValue + rew, actionValue)
#loss = T.mean((output[T.arange(batchSize), action.reshape((-1,))] - (rew + (disc * actionBestTargetValue)))**2)
target = (rew + disc * output_target[T.arange(batchSize), action_target])
network = output[T.arange(batchSize), action]
diff = target - network
loss = 0.5 * diff ** 2
loss = T.mean(loss)
grad = T.grad(loss, params)
updates = lasagne.updates.sgd(grad, params, learning_rate=learningRate)

# theano functions
f_train = theano.function([X_sym, action, Y_sym, rew, disc, action_target], loss, updates=updates, allow_input_downcast=True)
f_predict = theano.function([X_sym], actionBest, allow_input_downcast=True)
f_predict_target_value = theano.function([Y_sym], actionBestTargetValue, allow_input_downcast=True)
q_val = theano.function([X_sym, action], actionValue, allow_input_downcast=True)
q_bestval = theano.function([X_sym], actionValueBest, allow_input_downcast=True)
q_vals = theano.function([X_sym], actionValues, allow_input_downcast=True)
f_predict_target = theano.function([Y_sym], actionBestTarget, allow_input_downcast=True)

# helper functions
grad_calc = theano.function([X_sym, action, Y_sym, rew, disc, action_target], grad)
output_calc = theano.function([X_sym], output)
network_calc = theano.function([X_sym, action], network)
target_calc = theano.function([Y_sym, rew, disc, action_target], target)

################
# globals for training and evaluation
####################

replayMemory = []
epsilon = 0.8
iterationsToTrain = 0

# globals for evaluation while training
amountOfEpisodesTraining = 0
amountOfMovesTraining = 0
cumulatedLossTrain = 0
averageLossTraining = []

cumulatedLossEpisode = 0
cumulatedLossEpisodeList = []

bestQValues = []

# globals for evaluation in between training
amountOfEpisodesTest = 0
lengthEpisodeList = []
accumulatedLengthEpisode = 0
averageLengthEpisode = 0
averageLengthEpisodeList = []
amountOfInfiniteLoops = 0
amountOfInfiniteLoopsList = []

amountOfLoops = 0
timePerLoop = []

####################
# Training methods
###################

# epsilon-greedy, return the chosen move. high epsilon for random moves
def performAction(epsilon, state):
	randomNumber = random.uniform(0,1)
	if randomNumber < epsilon:
		move = random.randint(0,8)
	else:
		move = f_predict(state)
		# to get rid of the array
		move = move[0]

	# performs move only if move is legal
	return move, movePieceIfPossible(move)


# perform action and get reward
def fillRM():
	global all_param_valueslist
	global bestQValues
	global replayMemory
	global amountOfMovesTraining
	global epsilon
	global cumulatedLossEpisode
	global learningRate

	amountOfMovesTraining += 1
	#epsilon = 1 - ((float(amountOfMovesTraining)-((1/10)*iterationsToTrain))/iterationsToTrain)
	if amountOfMovesTraining % 6000 == 0 and amountOfMovesTraining > 0:
		epsilon = max(0.1, epsilon - 0.1)

	# if amountOfMovesTraining % 6000 == 0 and amountOfMovesTraining > 0:
	# 	learningRate -= 0.001

	state = board.flatten()
	move, moveLegal = performAction(epsilon, np.array([state]))
	# if K made a move, only then k makes a move too
	if moveLegal == True:
		#moveBlackKingRandom()
		moveBlackKing(maxNodesEngine)
	newState = board.flatten()
	if moveLegal == False and stalemate() == False:
		reward = -10
		discount = 0.99
	else:
		if isTerminalState() == 'won':
			reward = 10
			discount = 0
		if isTerminalState() == 'drawn':
			reward = -10
			discount = 0
		if isTerminalState() == False:
			reward = -1
			discount = 0.99

	# save experience in RM
	if len(replayMemory) < 10000:
		replayMemory.append([state, move, reward, newState, discount])
	else:
		replayMemory.pop(0)
		replayMemory.append([state, move, reward, newState, discount])

	#for evaluation
	bestQValue = q_bestval(np.array([state]))
	bestQValue = bestQValue[0]
	bestQValues.append(bestQValue)

# pick random states from RM and train
def trainOnRM():
	global replayMemory
	global cumulatedLossEpisode
	global batchSize

	if len(replayMemory) > 100:
		states = []
		moves = []
		rewards = []
		newStates = []
		discounts = []
		targetMoves = []
		for sample in range(batchSize):
			randomNumber = random.randint(0, len(replayMemory) - 1)
			states.append(replayMemory[randomNumber][0])
			moves.append(replayMemory[randomNumber][1])
			rewards.append(replayMemory[randomNumber][2])
			newStates.append(replayMemory[randomNumber][3])
			discounts.append(replayMemory[randomNumber][4])
			targetMove = f_predict_target(np.expand_dims(replayMemory[randomNumber][3],0))
			targetMove = targetMove[0]
			targetMoves.append(targetMove)
			# update params
		#print network_calc(states, moves)
		#print target_calc(newStates, rewards, discounts)
		#print discounts
		#print moves
		#print targetMoves
		loss = f_train(states, moves, newStates, rewards, discounts, targetMoves)
		cumulatedLossEpisode += loss

# the main training loop
def trainLoop(iterations):
	global replayMemory

	global amountOfEpisodesTraining
	global cumulatedLossTrain
	global averageLossTraining
	global cumulatedLossEpisode
	global cumulatedLossEpisodeList

	global bestQValues
	counter = 0

	for i in range(iterations):
		amountOfEpisodesTraining += 1
		cumulatedLossEpisode = 0

		# update target network
		counter += 1
		if counter == 100:
			all_param_values = lasagne.layers.get_all_param_values(l_out)
			lasagne.layers.set_all_param_values(l_out_target, all_param_values)
			counter = 0

		# to play games
		boardSetupPawn()
		while isTerminalState() == False:
			fillRM()
			trainOnRM()
		# for evaluation
		cumulatedLossTrain += cumulatedLossEpisode
		cumulatedLossEpisodeList.append(cumulatedLossEpisode)
		averageLossTraining.append(cumulatedLossTrain/amountOfEpisodesTraining)

####################
# Test and evaluation methods
###################

# saves board in file
def moveBothKingsSaveBoard():
	global maxNodesEngine
	state = np.array([board.flatten()])
	# both kings move
	move, moveLegal = performAction(0, state)
	while moveLegal == False and stalemate() == False:
		move, moveLegal = performAction(1, state)
	#boardToImage()
	printBoard()
	moveBlackKing(maxNodesEngine)
	#boardToImage()
	printBoard()

# moves both kings
def moveBothKingsTest(learned):
	global maxNodesEngine
	state = np.array([board.flatten()])
	# both kings move
	if learned:
		move, moveLegal = performAction(0, state)
	else:
		move, moveLegal = performAction(1, state)
	while moveLegal == False and stalemate() == False:
		move, moveLegal = performAction(1, state)
	moveBlackKing(maxNodesEngine)
	#moveBlackKingRandom()

# to test, save option to save image of board in file, learned option to use NN or not
def test(iterations, save, learned, difficulty):
	amountOfInfiniteLoops = 0
	wins = 0
	winningPositions = 0.0
	draws = 0
	drawnPositions = 0.0

	wonInWinningPosition = 0.0
	wonInDrawnPosition = 0.0

	movesToWin = 0.0
	for i in range(iterations):
		if difficulty == 'easy':
			boardSetupPawnEasy()
		if difficulty == 'medium':
			boardSetupPawnMedium()
		if difficulty == 'hard':
			boardSetupPawnHard()
		if difficulty == 'None':
			boardSetupPawn()
		score = boardEvaluation()
		if score == 1:
			winningPositions += 1
		if score == 0:
			drawnPositions += 1

		counter = 0
		lengthEpisode = 0
		while isTerminalState() == False and counter < 50:
			if save:
				qVals = q_vals(np.array([board.flatten()]))
				print np.sort(qVals)
				print np.argsort(qVals, axis=0)
				print '-' * 50
				moveBothKingsSaveBoard()
				if counter == 30:
					amountOfInfiniteLoops += 1
					print 'INFINITE LOOP!'
					print '-' * 50
					print '-' * 50
			else:
				moveBothKingsTest(learned)

			counter += 1
			lengthEpisode += 1

		if isTerminalState() == 'won':
			if score == 1:
				wonInWinningPosition += 1
			if score == 0:
				wonInDrawnPosition += 1
			wins += 1
			movesToWin += lengthEpisode
		if isTerminalState() == 'drawn':

			draws += 1

		#print lengthEpisode
	print winningPositions
	print drawnPositions
	print '-' * 50
	return amountOfInfiniteLoops, movesToWin, wins, wonInWinningPosition, winningPositions, wonInDrawnPosition, drawnPositions



drawnPositionsEv = 0.0
wonInDrawnPositionEv = 0.0
drawsEv = 0


averageLengthToWinList = []
accumulatedLengthToWin = 0.0
winsEv = 0

percentageWoninWinOverAllWinPosList = []
winningPositionsEv = 0.0
wonInWinningPositionEv = 0.0

# to evaluate during training
def evaluateDuringTraining(iterations):

	global drawnPositionsEv
	global wonInDrawnPositionEv
	global drawsEv

	global averageLengthToWinList
	global accumulatedLengthToWin
	global winsEv

	global percentageWoninWinOverAllWinPosList
	global winningPositionsEv
	global wonInWinningPositionEv

	global maxNodesEngine

	amountOfInfiniteLoops = 0
	amountOfMoves = 0

	for i in range(iterations):
		boardSetupPawn()
		score = boardEvaluation()
		if score == 1:
			winningPositionsEv += 1
		if score == 0:
			drawnPositionsEv += 1

		counter = 0
		lengthEpisode = 0
		while isTerminalState() == False and counter < 50:
			counter += 1
			amountOfMoves += 1
			lengthEpisode += 1

			moveBothKingsTest(True)

		if isTerminalState() == 'won':
			if score == 1:
				wonInWinningPositionEv += 1
			if score == 0:
				wonInDrawnPositionEv += 1
			winsEv += 1
			accumulatedLengthToWin += lengthEpisode
			averageLengthToWinList.append(accumulatedLengthToWin / winsEv)
		if isTerminalState() == 'drawn':
			drawsEv += 1

		#lengthEpisodeList.append(lengthEpisode)
		if score == 1:
			percentageWoninWinOverAllWinPosList.append(wonInWinningPositionEv / winningPositionsEv)


		#amountOfInfiniteLoopsList.append(amountOfInfiniteLoops)


def evaluateDuringTraining2(iterations):
	global amountOfEpisodesTest
	global lengthEpisodeList
	global accumulatedLengthEpisode
	global averageLengthEpisodeList
	global averageLengthEpisode
	global amountOfInfiniteLoops
	global amountOfInfiniteLoopsList
	global maxNodesEngine

	for i in range(iterations):
		amountOfEpisodesTest += 1
		boardSetupPawn()
		counter = 0
		lengthEpisode = 0.0
		while isTerminalState() == False and counter < 50:
			moveBothKingsTest(maxNodesEngine)
			counter += 1
			lengthEpisode += 1
			if counter == 49:
				amountOfInfiniteLoops += 1

		lengthEpisodeList.append(lengthEpisode)
		accumulatedLengthEpisode += lengthEpisode
		averageLengthEpisodeList.append(accumulatedLengthEpisode/amountOfEpisodesTest)
		amountOfInfiniteLoopsList.append(amountOfInfiniteLoops)

def evaluate():
	# while training
	global amountOfMovesTraining
	global amountOfEpisodesTraining
	global cumulatedLossEpisodeList
	global averageLossTraining
	# while evaluating during training
	global amountOfEpisodesTest
	global lengthEpisodeList
	global averageLengthEpisodeList
	global amountOfInfiniteLoops
	global amountOfInfiniteLoopsList

	global bestQValues
	global amountOfLoops
	global timePerLoop

	global percentageWoninWinOverAllWinPosList
	global averageLengthToWinList

	#evaluation
	# plot q vals of best move in all episodes
	amountOfEpisodesTrainingList = np.arange(amountOfEpisodesTraining)
	amountOfMovesTrainingList = np.arange(amountOfMovesTraining)

	plt.figure()
	plt.title('Q values')
	plt.xlabel('Moves')
	plt.ylabel('Highest Q value of position')
	plt.plot(amountOfMovesTrainingList, bestQValues)
	plt.savefig('Plots/bestQ.png')
	#plt.show()
	# plot cost per episode
	# plt.figure()
	# plt.plot(amountOfEpisodesTrainingList, cumulatedLossEpisodeList)
	plt.figure()
	plt.title('Loss')
	plt.xlabel('Games')
	plt.ylabel('Average loss')
	plt.plot(amountOfEpisodesTrainingList[500:], averageLossTraining[500:])
	plt.savefig('Plots/AvLoss.png')
	# amountOfEpisodesTestList = np.arange(amountOfEpisodesTest)
	# plt.figure()
	# plt.plot(amountOfEpisodesTestList[30:], lengthEpisodeList[30:])
	# plt.savefig('Plots/moves.png')
	# plt.figure()
	# plt.plot(amountOfEpisodesTestList, averageLengthEpisodeList)
	# plt.savefig('Plots/avmoves.png')
	# plt.figure()
	# plt.plot(amountOfEpisodesTestList, amountOfInfiniteLoopsList)
	# plt.savefig('Plots/infloops.png')


	amountOfWinsList = np.arange(winsEv)
	amountOfWinninPosList = np.arange(winningPositionsEv)

	plt.figure()
	plt.title('Winning-rate during training')
	plt.xlabel('Winning positions')
	plt.ylabel('Average won in winning positions in percentage')
	plt.plot(amountOfWinninPosList, percentageWoninWinOverAllWinPosList)
	plt.savefig('Plots/winningRate.png')

	plt.figure()
	plt.title('Length to win during training')
	plt.xlabel('Wins')
	plt.ylabel('Average length to win')
	plt.plot(amountOfWinsList, averageLengthToWinList)
	plt.savefig('Plots/lengthToWin.png')

	plt.figure()
	amountOfLoopsList = np.arange(amountOfLoops)
	plt.title('Time per loop')
	plt.xlabel('Games / 100')
	plt.ylabel('Time')
	plt.plot(amountOfLoopsList, timePerLoop)
	plt.savefig('Plots/time.png')
	plt.show()

modelFile = 'Models/SavedModel.txt'
def saveModel():
	global modelFile
	all_param_values = lasagne.layers.get_all_param_values(l_out)
	with open(modelFile, 'w') as f:
		pickle.dump(all_param_values, f)

def loadModel():
	global modelFile
	with open(modelFile, 'r') as f:
		loadedParams = pickle.load(f)
	lasagne.layers.set_all_param_values(l_out, loadedParams)
	lasagne.layers.set_all_param_values(l_out_target, loadedParams)


# # unused
# global amountOfEpisodesTest

# global lengthEpisodeList
# global accumulatedLengthEpisode
# global averageLengthEpisodeList

# global averageLengthEpisode
# global amountOfInfiniteLoops
# global amountOfInfiniteLoopsList

# cumulatedLossEpisodeList
# amountOfInfiniteLoopsList
# #

# global drawnPositionsEv
# global wonInDrawnPositionEv
# global drawsEv
#
# global averageLengthToWinList
# global accumulatedLengthToWin
# global winsEv
#
# global percentageWoninWinOverAllWinPosList
# global winningPositionsEv
# global wonInWinningPositionEv

outputFile1  = 'Models/SavedReplaymemory.npy'
outputFile2  = 'Models/amountOfMovesTraining.npy'
outputFile3  = 'Models/amountOfEpisodesTraining.npy'

outputFile5  = 'Models/averageLossTraining.npy'

outputFile9  = 'Models/bestQValues.npy'
outputFile10  = 'Models/epsilon.npy'

outputFile11 = 'Models/iterationsToTrain.npy'
outputFile12 = 'Models/cumulatedLossTrain.npy'
outputFile13 = 'Models/cumulatedLossEpisode.npy'
outputFile14 = 'Models/accumulatedLengthEpisode.npy'
outputFile15 = 'Models/averageLengthEpisode.npy'

outputFile18 = 'Models/amountOfLoops.npy'
outputFile19 = 'Models/timePerLoop.npy'


outputFile4  = 'Models/drawnPositionsEv.npy'
outputFile6  = 'Models/wonInDrawnPositionEv.npy'
outputFile7  = 'Models/drawsEv.npy'

outputFile8  = 'Models/averageLengthToWinList.npy'
outputFile16 = 'Models/accumulatedLengthToWin.npy'
outputFile17 = 'Models/winsEv.npy'

outputFile21 = 'Models/percentageWoninWinOverAllWinPosList.npy'
outputFile22 = 'Models/winningPositionsEv.npy'
outputFile23 = 'Models/wonInWinningPositionEv.npy'

def saveEvaluation():
	global replayMemory

	global amountOfMovesTraining
	global amountOfEpisodesTraining
	global cumulatedLossEpisodeList
	global averageLossTraining
	global amountOfEpisodesTest
	global lengthEpisodeList
	global averageLengthEpisodeList
	global bestQValues

	global epsilon

	global iterationsToTrain
	global cumulatedLossTrain
	global cumulatedLossEpisode
	global accumulatedLengthEpisode
	global averageLengthEpisode
	global amountOfInfiniteLoops
	global amountOfInfiniteLoopsList
	global amountOfLoops
	global timePerLoop

	global drawnPositionsEv
	global wonInDrawnPositionEv
	global drawsEv

	global averageLengthToWinList
	global accumulatedLengthToWin
	global winsEv

	global percentageWoninWinOverAllWinPosList
	global winningPositionsEv
	global wonInWinningPositionEv

	np.save(outputFile1, replayMemory)
	np.save(outputFile2, amountOfMovesTraining)
	np.save(outputFile3, amountOfEpisodesTraining)
	np.save(outputFile4, drawnPositionsEv)
	np.save(outputFile5, averageLossTraining)
	np.save(outputFile6, wonInDrawnPositionEv)
	np.save(outputFile7, drawsEv)
	np.save(outputFile8, averageLengthToWinList)
	np.save(outputFile9, bestQValues)
	np.save(outputFile10, epsilon)
	np.save(outputFile11, iterationsToTrain)
	np.save(outputFile12, cumulatedLossTrain)
	np.save(outputFile13, cumulatedLossEpisode)
	np.save(outputFile14, accumulatedLengthEpisode)
	np.save(outputFile15, averageLengthEpisode)
	np.save(outputFile16, accumulatedLengthToWin)
	np.save(outputFile17, winsEv)
	np.save(outputFile18, amountOfLoops)
	np.save(outputFile19, timePerLoop)
	np.save(outputFile21, percentageWoninWinOverAllWinPosList)
	np.save(outputFile22, winningPositionsEv)
	np.save(outputFile23, wonInWinningPositionEv)


def loadEvaluation():
	global replayMemory

	global amountOfMovesTraining
	global amountOfEpisodesTraining
	global cumulatedLossEpisodeList
	global averageLossTraining
	global amountOfEpisodesTest
	global lengthEpisodeList
	global averageLengthEpisodeList
	global bestQValues

	global epsilon

	global iterationsToTrain
	global cumulatedLossTrain
	global cumulatedLossEpisode
	global accumulatedLengthEpisode
	global averageLengthEpisode
	global amountOfInfiniteLoops
	global amountOfInfiniteLoopsList
	global amountOfLoops
	global timePerLoop

	global drawnPositionsEv
	global wonInDrawnPositionEv
	global drawsEv

	global averageLengthToWinList
	global accumulatedLengthToWin
	global winsEv

	global percentageWoninWinOverAllWinPosList
	global winningPositionsEv
	global wonInWinningPositionEv

	replayMemory = np.ndarray.tolist(np.load(outputFile1))
	amountOfMovesTraining = np.ndarray.tolist(np.load(outputFile2))
	amountOfEpisodesTraining = np.ndarray.tolist(np.load(outputFile3))
	drawnPositionsEv = np.ndarray.tolist(np.load(outputFile4))
	averageLossTraining = np.ndarray.tolist(np.load(outputFile5))
	wonInDrawnPositionEv = np.ndarray.tolist(np.load(outputFile6))
	drawsEv = np.ndarray.tolist(np.load(outputFile7))
	averageLengthToWinList = np.ndarray.tolist(np.load(outputFile8))
	bestQValues = np.ndarray.tolist(np.load(outputFile9))
	epsilon = np.ndarray.tolist(np.load(outputFile10))
	iterationsToTrain = np.ndarray.tolist(np.load(outputFile11))
	cumulatedLossTrain = np.ndarray.tolist(np.load(outputFile12))
	cumulatedLossEpisode = np.ndarray.tolist(np.load(outputFile13))
	accumulatedLengthEpisode = np.ndarray.tolist(np.load(outputFile14))
	averageLengthEpisode = np.ndarray.tolist(np.load(outputFile15))
	accumulatedLengthToWin = np.ndarray.tolist(np.load(outputFile16))
	winsEv = np.ndarray.tolist(np.load(outputFile17))
	amountOfLoops = np.ndarray.tolist(np.load(outputFile18))
	timePerLoop = np.ndarray.tolist(np.load(outputFile19))

	percentageWoninWinOverAllWinPosList = np.ndarray.tolist(np.load(outputFile21))
	winningPositionsEv = np.ndarray.tolist(np.load(outputFile22))
	wonInWinningPositionEv = np.ndarray.tolist(np.load(outputFile23))

def train():
	global amountOfLoops
	global timePerLoop
	global amountOfMovesTraining
	global randomMoves
	global amountOfEpisodesTraining
	global averageLossTraining
	global cumulatedLossTrain
	global amountOfEpisodesTraining

	global winningPositionsEv
	global drawnPositionsEv
	#global iterationNumber
	#
	loadModel()
	# probleemje met np inladen enzo. zie f predict
	loadEvaluation()
	amountOfLoopsJ = 50
	amountOfLoops += amountOfLoopsJ
	iterations = 102

	testGames = 1000

	startTime = time.clock()
	#amountOfInfiniteLoopsBase, AmountOfMovesBase, winsBase, winsInWinningBase, winningPosBase, winsInDrawnBase, drawnPosBase = test(testGames, False, False, 'None')
	randomMoves = 0.0
	for i in range(amountOfLoopsJ):
		t0 = time.clock()
		trainLoop(iterations)
		print i
		print "epsilon = ", epsilon
		print "average loss = ", cumulatedLossTrain / amountOfEpisodesTraining
		print "rate of random moves by SunFish per training move= ", randomMoves / amountOfMovesTraining
		print "amount of games where SunFish did random move = ", randomMoves / amountOfEpisodesTraining

		evaluateDuringTraining(20)
		print time.clock() - t0, "seconds process time"
		timePerLoop.append((time.clock() - t0))

		saveModel()
		saveEvaluation()
		print '-' * 50

	print "rate of random moves by SunFish per training move= ", randomMoves/amountOfMovesTraining
	print "amount of games where SunFish did random move = ", randomMoves/amountOfEpisodesTraining

	#amountOfInfiniteLoops, AmountOfMoves, wins, winsInWinning, winningPos, winsInDrawn, drawnPos = test(testGames, False, True, 'None')
	# amountOfInfiniteLoopsEasy, AmountOfMovesEasy, winsEasy, winsInWinningEasy, winningPosEasy, winsInDrawnEasy, drawnPosEasy = test(testGames, False, True, 'easy')
	# amountOfInfiniteLoopsMedium, AmountOfMovesMedium, winsMedium, winsInWinningMedium, winningPosMedium, winsInDrawnMedium, drawnPosMedium = test(testGames, False, True, 'medium')
	# amountOfInfiniteLoopsHard, AmountOfMovesHard, winsHard, winsInWinningHard, winningPosHard, winsInDrawnHard, drawnPosHard = test(testGames, False, True, 'hard')


	# pretty print
	# print "ALL POSITIONS:"
	# test(15, True, True, 'None')
	# print "EASY POSITIONS:"
	# test(15, True, True, 'easy')
	# print "MEDIUM POSITIONS:"
	# test(15, True, True, 'medium')
	# print "HARD POSITIONS:"
	# test(15, True, True, 'hard')

	endTime = time.clock()
	print '-' * 50
	print endTime - startTime, "seconds total time"
	print '-' * 50
	# print "infinite loops baseline = ", amountOfInfiniteLoopsBase
	# print "moves till won baseline = " , AmountOfMovesBase/winsBase
	# print "wins in winning positions baseline = ", winsInWinningBase/winningPosBase
	# print "wins in drawn positions baseline = ", winsInDrawnBase/drawnPosBase
	# print '-' * 50
	# print "infinite loops after learning= ", amountOfInfiniteLoops
	# print "moves till won after learning = " , AmountOfMoves/wins
	# print "wins in winning positions after learning = ", winsInWinning/winningPos
	# print "wins in drawn positions after learning = ", winsInDrawn/drawnPos

	# print '-' * 50
	# print "EASY results"
	# print "infinite loops after learning= ", amountOfInfiniteLoopsEasy
	# print "moves till won after learning = " , AmountOfMovesEasy/winsEasy
	# print "wins in winning positions after learning = ", winsInWinningEasy/winningPosEasy
	# print "wins in drawn positions after learning = ", winsInDrawnEasy/(drawnPosEasy +1)
	# print "ratio of winning positions= ", winningPosEasy/testGames
    #
	# print '-' * 50
	# print "MEDIUM results"
	# print "infinite loops after learning= ", amountOfInfiniteLoopsMedium
	# print "moves till won after learning = " , AmountOfMovesMedium/winsMedium
	# print "wins in winning positions after learning = ", winsInWinningMedium/winningPosMedium
	# print "wins in drawn positions after learning = ", winsInDrawnMedium/drawnPosMedium
	# print "ratio of winning positions= ", winningPosMedium/testGames
    #
	# print '-' * 50
	# print "HARD results"
	# print "infinite loops after learning= ", amountOfInfiniteLoopsHard
	# print "moves till won after learning = " , AmountOfMovesHard/winsHard
	# print "wins in winning positions after learning = ", winsInWinningHard/winningPosHard
	# print "wins in drawn positions after learning = ", winsInDrawnHard/drawnPosHard
	# print "ratio of winning positions= ", winningPosHard/testGames

	#evaluate()

train()


# nu opeens weer sneller? Wat gebeurt er?

# overmorgen x quars fixen en elke iteratie plotje printen INZICHT, dan kun je bugs wel vinden
# misschien wel a te groot ofzo.


# Werkt het nu? Volgens mij wel. Maar het lijkt wel alsof hij minder wint als ie langer traint.
# Voor inzicht, amount of winns in winning pos tracking, daar gaat het om DONE
# amount of moves to win DONE
# av amount of infinite loops
# sunfish doet hele rare zetten.
# Ik denk dat in verliezende stellingen sunfish ook slechtere zetten doet?
# random zetten veel minder bij langere max nodes
# ik vind de baseline wel erg hoog, even checken of wel klopt.

# als ik morgen die plotjes van leren heb kan ik pas langer trainen, nu gewoon geen inzicht.


# bij de uiteindelijke resultaten de testen veel langer. Dan nog wat experimentjes doen
# voor laatste paar procentjes, met super computer. Maak mooie plaatjes van resultaten
# kan ook in 4x4 bord. Dan sunfish ook veel langer



# dan hoef ik alleen nog ev via supercomputer te doen.



# PROBLEEM: bij 2000 zoek dan na 18 ofzo NONE van sunfish
# nu even proberen met 200 zoek, zodat ik iets heb morgen.

# winrate gaat omlaag en is eerst heel erg hoog.

# gezien het gifje heeft hij niet geleerd om samen te werken met de pion.

# duurt langer als ie langer heeft getraind, makes sense?

# lijkt ook wel alsof soms gewoon 6 keer zo langzaam ofzo, zonder duidelijke reden.

# results laten zien, grafiekjes laten zien, gif laten zien
# probleem dat hij niet goed samenwerkt: hoe kan dat?
# probleem van NONE moves, hoe debuggen?

# restart all params ofzo, gewoon train() vaker aanroepen? Of anders elke 100 i even opnieuw aan
# zetten.

# opdelen in easy and hard positions.

# sunfish kan het ook slecht doen in posiites die voor hem verloren zijn, dus dat het best makkelijk i
# om te winnen in winning pos, en in drawn opeens heel moeilijk.

# nu moet ik dus ev beter opslaan, zoals epsilon, even kijken of RM uitmaakt. Even
# opletten met RM inladen en foutje. Misschien ligt het aan expand dims, of heb ik het niet
# meer gecheckt na batches? worth trying.

# even kijken of hij nu weer na 13 stopt (duurt lang). nu lijkt loadev niet fout te gaan, maar misschien
# gaat ie stiekem fout. anders RM opnieuw vullen, kan ook. (mag denk ik niet echt)

# als dit werkt, dan werkt alles opzich. Ik hoef mijn resultaten niet meer te verbeteren
# geen tijd meer voor. Dan alles goed opschrijven.
# experiments ook wat ik eerst deed met wat getalletjes, hoeft niet lang, maar wel
# een soort opbouw, zodat mensen mijn approach na kunnen doen.


# misschien die nieuwe evaluatie weer opslaan in files en plotten.

# let it train in hard positions and see how it performs? Should the network be bigger?

# moet ik nog experimenteren met de netwerk grootte? Dan toch iets schrijven dat steeds
# opnieuw python reset?

# moet de learning rate misschien kleiner? (nee dat is alleen als loss niet verder omlaag
# gaat.

# kan ook steeds evaluaeren zonder show, zodat hij het in de plaatjes opslaat, goed voor
# inzicht.

# misschien programma stoppen als NONE move




# moet eigenlijk meer inzicht in hoe hij het doet, door te kijken naar de game.
# Als ik probeer die 3 level resultaten zo hoog mogelijk te krijgen moet ik ook
# die plotten, eigenlijk. Wees zeker of hij nog leert.



# Discussie: ik zou hem ook verliezende stellingen moeten geven, dan kan je zien of
# hij meer wint dan verliest.



#
#
# boardSetupPawn2()
# printBoard()
# print board
# for i in range(3):
# 	state = board.flatten()
# 	performAction(epsilon, np.array([state]))
# 	printBoard()
# 	print board
# 	moveBlackKing(200)
# 	printBoard()
# 	print board
