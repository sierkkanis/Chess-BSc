import numpy as np
import random
import theano
import theano.tensor as T
import lasagne
from lasagne.layers import InputLayer, DenseLayer
from lasagne.nonlinearities import LeakyRectify

board = 0
board2D = 0
pieceDict = {'K' : 0, 'k' : 1, 'P' : 2}
turn = True
halfmoves = 0
reward = 0
discount = 0.9

# for board representation
noPieceNumber = -1.0/64.0
PieceNumber = 63.0/64.0

# reset all board parameters
def boardInit(amountPieces):
	global board
	global turn
	global halfmoves
	global reward
	global noPieceNumber
	board = np.empty(64*amountPieces).reshape((amountPieces,8,8))
	for x in range(len(board)):
		for y in range(len(board[x])):
			board[x][y] = noPieceNumber
	turn = True
	halfmoves = 0
	reward = 0

# set piece, doesn't remove the old
def setPiece(piece, x, y):
	global board
	global pieceDict
	global PieceNumber
	board[pieceDict.get(piece)][y][x] = PieceNumber

def setPieceRandom(piece):
	xRandom = random.randint(0,7)
	yRandom = random.randint(0,7)
	setPiece(piece, xRandom, yRandom)

def movePiece(piece, x, y):
	global board
	global pieceDict
	global turn
	global halfmoves
	global PieceNumber
	global noPieceNumber
	for n in range(x):
		for m in range(y):
			board[pieceDict.get(piece)][m][n] = noPieceNumber
	board[pieceDict.get(piece)][y][x] = PieceNumber
	halfmoves = halfmoves + 1
	if turn == True:
		turn = False
	else:
		turn = True

# moves whiteKing into direction
def moveKing(direction):
	global board
	global reward
	global PieceNumber
	global noPieceNumber
	x, y = getCoor('K')
	if direction == 0 and y != 0:
		board[0][y-1][x] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	if direction == 1 and y != 0 and x != 0:
		board[0][y-1][x-1] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	if direction == 2 and y != 0 and x != 7:
		board[0][y-1][x+1] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	if direction == 3 and x != 0:
		board[0][y][x-1] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	if direction == 4 and x != 7:
		board[0][y][x+1] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	if direction == 5 and y != 7:
		board[0][y+1][x] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	if direction == 6 and y != 7 and x != 0:
		board[0][y+1][x-1] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	if direction == 7 and y != 7 and x != 7:
		board[0][y+1][x+1] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	else: 
		return False

# whiteking
def moveKingRandom():
	rand = random.randint(0,7)
	moveKing(rand)
	return rand

def boardTo2D():
	global board2D
	global board
	board2D = np.chararray((8,8))
	board2D[:] = '.'
	pieceDict = {0 : 'K', 1 : 'k', 2: 'P'}
	for piece in range(len(board)):
		x, y = getCoor(pieceDict.get(piece))
		board2D[y][x] = pieceDict.get(piece)

def printBoard():
	global board2D
	global turn
	global halfmoves
	global reward
	boardTo2D()
	print board2D
	print ' '
	"""
	print 'turn = '
	print turn
	#print 'position is legal = '
	#print isLegal()
	print 'halfmoves = '
	print halfmoves
	print 'terminal state '
	print isTerminalState()
	print 'reward = '
	print reward
	"""

def getCoor(piece):
	global board
	piece = pieceDict.get(piece)
	for y in range(len(board[piece])):
		for x in range(len(board[piece][y])):
			if board[piece][y][x] > 0:
				return x, y

# for black king
def inCheck():
	global board
	xP, yP = getCoor('P')
	xk, yk = getCoor('k')
	if yk == yP -1 and (xk == xP - 1 or xk == xP + 1):
		return True
	else: 
		return False

# add pieces on top of each other
# if currentstate is legal, if kings touch oneanother (or are on top of eachother, 
# can only happen with setup, so re place later)
def isLegal():
	global turn
	# if turn == True and inCheck():
	# 	return False
	xk, yk = getCoor('k')
	xK, yK = getCoor('K')
	if ((yk == yK -1 and (xk == xK - 1 or xk == xK or xk == xK + 1))
		or (yk == yK and (xk == xK - 1 or xk == xK or xk == xK + 1))
		or (yk == yK +1 and (xk == xK - 1 or xk == xK or xk == xK + 1))
		or (xk == xK and yk == yK)):
		return False
	else:
		return True

# if king's at the end of the board
def isTerminalState():
	global reward
	x, y = getCoor('K')
	if y == 0:
		#reward += 10
		return True
	else:
		return False

# like numpy reshape, returns flattend array of 2d board, with shape (1,64)
def flattenChessboard():
	global board
	amountPieces = len(board)
	newArray = np.zeros((1,64*amountPieces))
	array = []
	for piece in range(len(matrix)):
		for row in range(len(matrix[piece])):
			for number in range(len(matrix[piece][row])):
				newArray[0][piece*64+row*8+number] = matrix[piece][row][number]
	return newArray

"""
notes:

"""

"""
NETWORK
"""

# linear regression (?)
l_in = lasagne.layers.InputLayer((1, 128))
custom_rectify = LeakyRectify(1)
l_out = lasagne.layers.DenseLayer(l_in, num_units=8, nonlinearity = custom_rectify)
all_param_values = lasagne.layers.get_all_param_values(l_out)

# same but for target, copy weights from trainable network
l_in_target = lasagne.layers.InputLayer((1, 128))
l_out_target = lasagne.layers.DenseLayer(l_in_target, num_units=8, nonlinearity = custom_rectify)
lasagne.layers.set_all_param_values(l_out_target, all_param_values)


X_sym = T.matrix()
Y_sym = T.matrix()
rew = T.scalar()
disc = T.scalar()
action = T.iscalar()

# trainable network
output = lasagne.layers.get_output(l_out, X_sym)
params = lasagne.layers.get_all_params(l_out)
actionBest = output.argmax()
actionValue = output[0][action]
actionValues = output[0]

# target network
output_target = lasagne.layers.get_output(l_out_target, Y_sym)
params_target = lasagne.layers.get_all_params(l_out_target)
actionBestTargetValue = output_target.max()

# general lasagne
loss = lasagne.objectives.squared_error(disc*actionBestTargetValue + rew, actionValue)
grad = T.grad(loss, params)
updates = lasagne.updates.sgd(grad, params, learning_rate=0.5)

# theano functions
f_train = theano.function([X_sym, action, Y_sym, rew, disc], loss, updates=updates, allow_input_downcast=True)
f_predict = theano.function([X_sym], actionBest, allow_input_downcast=True)
f_predict_target_value = theano.function([Y_sym], actionBestTargetValue, allow_input_downcast=True)
q_val = theano.function([X_sym, action], actionValue)
q_vals = theano.function([X_sym], actionValues)

# helper functions
grad_calc = theano.function([X_sym, action, Y_sym, rew, disc], grad)
output_calc = theano.function([X_sym], output)

# epsilon-greedy, return the chosen move. high epsilon for random moves
def performAction(epsilon, state):
	randomNumber = random.uniform(0,1)
	moveOnBoard = False
	moveLegal = False
	redoMoveDict = {0:5, 1:7, 2:6, 3:4, 4:3, 5:0, 6:2, 7:1}
	if randomNumber < epsilon:
		randMove = 0
		while moveOnBoard == False or moveLegal == False:
			randMove = random.randint(0,7)
			# performs move if move is on board
			moveOnBoard = moveKing(randMove)
			# redoes move when position is illegal
			if isLegal() == False:
				redoMove = redoMoveDict.get(randMove)
				moveKing(redoMove)
			else:
				moveLegal = True
		return randMove
	else:
		qVals = q_vals(state)
		sortedMoves = np.argsort(-qVals, axis=0)
		counter = 0
		predictedMove = 0
		# choses next best move after illegal move
		while moveOnBoard == False or moveLegal == False:
			predictedMove = sortedMoves[counter]
			moveOnBoard = moveKing(predictedMove)
			counter += 1
			if isLegal() == False:
				redoMove = redoMoveDict.get(predictedMove)
				moveKing(redoMove)
			else:
				moveLegal = True
		return predictedMove

def boardSetupTraining():
	boardInit(2)
	setPieceRandom('K')
	setPiece('k', 4, 2)
	if isLegal() == False:
		boardSetupTraining()

def boardSetupTest():
	boardInit(2)
	setPiece('K', 4, 6)
	setPiece('k', 4, 2)
	if isLegal() == False:
		boardSetupTest()

# the main training loop
def trainLoop(iterations):
	average_loss = 0
	reward = 0
	discount = 1
	counter = 0
	amountOfMoves = 0
	for i in range(iterations):
		boardSetupTraining()
		while isTerminalState() == False:
			state = flattenChessboard()
			# epsilon decreases over time, -(i*(1/2*iterations))
			move = performAction(1-(i*(1/iterations)), state)
			newState = flattenChessboard()
			if isTerminalState():
				loss = f_train(state, move, newState, 10, 0)
			else:
				loss = f_train(state, move, newState, 0, discount)
			amountOfMoves += 1

			"""
			#doesn't work yet
			counter += i
			if counter > 10:
				all_param_values = lasagne.layers.get_all_param_values(l_out)
				lasagne.layers.set_all_param_values(l_out_target, all_param_values)
				counter = 0
			"""

		#print amountOfMoves/(i+1)

		#q values and gradient test
		# for i in range(8):
		# 	gradcalcs = grad_calc(predictedMoveTarget, i, state, reward, discount)
		# 	a,b = gradcalcs

		# 	print '-'*80
		# 	print np.sum(a**2)
		# 	print np.sum(b**2)
		# 	print q_val(state, i)
		#print average_reward
		#print average_loss/i	
	#gradcalcs = grad_calc(predictedMoveTarget, move, state, reward, discount)
	#print gradcalcs	

# weights = l_out.W.get_value()
# print weights

trainLoop(300)

# weights2 = l_out.W.get_value()
# print weights2
# print weights2 - weights


# print weights
# print state
# print output_calc(state)
# print np.dot(state, weights)

#weights2 = l_out.W.get_value()
#print weights
#print weights2
#print weights-weights2

# to test
def test(iterations):
	amountOfMoves = 0
	for i in range(iterations):
		boardSetupTraining()
		#printBoard()
		counter = 0
		lengthEpisode = 0
		while isTerminalState() == False and counter < 70:
			state = flattenChessboard()
			performAction(0, state)
			counter += 1
			amountOfMoves += 1
			lengthEpisode += 1
			#printBoard()
			#print '-'*50
		print lengthEpisode
		#print amountOfMoves/(i+1)
		#print '-'*100

test(50)

"""
average loss zou omlaag moeten gaan
"""
