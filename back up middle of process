import numpy as np
import random
import theano
import theano.tensor as T
import lasagne
from lasagne.layers import InputLayer, DenseLayer
from lasagne.nonlinearities import LeakyRectify, sigmoid, rectify
import matplotlib.pyplot as plt
import cPickle as pickle
import os
import time

board = 0
board2D = 0
pieceDict = {'K' : 0, 'k' : 1, 'P' : 2}
pieceDict2 = {0 : 'K', 1 : 'k', 2: 'P'}
side = True
halfmoves = 0
reward = 0
discount = 0.9
piecesList = []
KCoors = (0,0)
kCoors = (0,0)
PCoors = (0,0)

# for board representation
noPieceNumber = -1.0/64.0
PieceNumber = 63.0/64.0

# reset all board parameters
def boardInit(amountPieces):
	global board
	global halfmoves
	global reward
	global noPieceNumber
	global piecesList
	global KCoors
	global kCoors
	global PCoors
	global side

	side = True
	KCoors = (0, 0)
	kCoors = (0, 0)
	PCoors = (0, 0)
	piecesList = []
	board = np.empty(64*amountPieces).reshape((amountPieces,8,8))
	for x in range(len(board)):
		for y in range(len(board[x])):
			board[x][y] = noPieceNumber
	halfmoves = 0
	reward = 0

# def getCoor(piece):
# 	global board
# 	piece = pieceDict.get(piece)
# 	for y in range(len(board[piece])):
# 		for x in range(len(board[piece][y])):
# 			if board[piece][y][x] > 0:
# 				return x, y

def switchSide():
	global side
	if side == True:
		side = False
	else:
		side = True

def getCoor(piece):
	global KCoors
	global kCoors
	global PCoors
	if piece == 'K':
		x, y = KCoors
		return x,y
	if piece == 'k':
		x, y = kCoors
		return x,y
	if piece == 'P':
		x, y = PCoors
		return x,y

# switches side when the coors are changed.
def setCoor(piece, x, y):
	global KCoors
	global kCoors
	global PCoors
	if piece == 'K':
		KCoors = (x, y)
	if piece == 'k':
		kCoors = (x, y)
	if piece == 'P':
		PCoors = (x, y)
	switchSide()

# set piece, doesn't remove the old, absolute coordinates
def setPiece(piece, x, y):
	global board
	global pieceDict
	global pieceDict2
	global PieceNumber
	global piecesList
	global KCoors
	global kCoors
	global PCoors
	if freeSquare(x, y):
		board[pieceDict.get(piece)][y][x] = PieceNumber
		setCoor(piece, x, y)
		piecesList.append(piece)
		return True
	else:
		return False

# checks whether square is free (only for white pieces, work-around on taking pieces)
def freeSquare(x, y):
	global side
	global piecesList
	if side == True:
		for piece in piecesList:
			xx, yy = getCoor(piece)
			if x == xx and y == yy:
				return False
		return True
	if side == False:
		return True

# checks if square is occupied by an enemy piece (now only pawn)
def enemyPiece(x,y, selfPiece):
	global side
	global piecesList
	if side == False:
		for piece in piecesList:
			xx, yy = getCoor(piece)
			if x == xx and y == yy and piece != selfPiece:
				return True
		return False
	if side == True:
		return False


def setPieceRandom(piece):
	onFreeSquare = False
	# checks whether square is free to add piece.
	while onFreeSquare == False:
		xRandom = random.randint(0,7)
		yRandom = random.randint(0,7)
		onFreeSquare = setPiece(piece, xRandom, yRandom)

def setPieceSemiRandom(piece):
	onFreeSquare = False
	# checks whether square is free to add piece.
	while onFreeSquare == False:
		xRandom = random.randint(0,3)
		yRandom = random.randint(0,3)
		onFreeSquare = setPiece(piece, xRandom, yRandom)

"""
not used
def movePiece(piece, x, y):
	global board
	global pieceDict
	global turn
	global halfmoves
	global PieceNumber
	global noPieceNumber
	for n in range(x):
		for m in range(y):
			board[pieceDict.get(piece)][m][n] = noPieceNumber
	board[pieceDict.get(piece)][y][x] = PieceNumber
	halfmoves = halfmoves + 1
	if turn == True:
		turn = False
	else:
		turn = True
"""
# moves king into direction
def moveKing(direction, king):
	global board
	global reward
	global PieceNumber
	global noPieceNumber
	global pieceDict
	x, y = getCoor(king)
	if direction == 0 and y != 0 and freeSquare(x, y-1):
		board[pieceDict.get(king)][y-1][x] = PieceNumber
		board[pieceDict.get(king)][y][x] = noPieceNumber
		setCoor(king, x, y-1)
		return True
	if direction == 1 and y != 0 and x != 0 and freeSquare(x-1, y-1):
		board[pieceDict.get(king)][y-1][x-1] = PieceNumber
		board[pieceDict.get(king)][y][x] = noPieceNumber
		setCoor(king, x-1, y-1)
		return True
	if direction == 2 and y != 0 and x != 7 and freeSquare(x+1, y-1):
		board[pieceDict.get(king)][y-1][x+1] = PieceNumber
		board[pieceDict.get(king)][y][x] = noPieceNumber
		setCoor(king, x+1, y-1)
		return True
	if direction == 3 and x != 0 and freeSquare(x-1, y):
		board[pieceDict.get(king)][y][x-1] = PieceNumber
		board[pieceDict.get(king)][y][x] = noPieceNumber
		setCoor(king, x-1, y)
		return True
	if direction == 4 and x != 7 and freeSquare(x+1, y):
		board[pieceDict.get(king)][y][x+1] = PieceNumber
		board[pieceDict.get(king)][y][x] = noPieceNumber
		setCoor(king, x+1, y)
		return True
	if direction == 5 and y != 7 and freeSquare(x, y+1):
		board[pieceDict.get(king)][y+1][x] = PieceNumber
		board[pieceDict.get(king)][y][x] = noPieceNumber
		setCoor(king, x, y+1)
		return True
	if direction == 6 and y != 7 and x != 0 and freeSquare(x-1, y+1):
		board[pieceDict.get(king)][y+1][x-1] = PieceNumber
		board[pieceDict.get(king)][y][x] = noPieceNumber
		setCoor(king, x-1, y+1)
		return True
	if direction == 7 and y != 7 and x != 7 and freeSquare(x+1, y+1):
		board[pieceDict.get(king)][y+1][x+1] = PieceNumber
		board[pieceDict.get(king)][y][x] = noPieceNumber
		setCoor(king, x+1, y+1)
		return True
	else: 
		return False

def checkIsLegal(piece, x,y):
	if isLegal():
		setCoor(piece, x, y)
		return True
	else:
		setCoor(piece, x, y)
		return False

# checks for illegal and off board moves
def tryToMoveKing(direction, king):
	global board
	global reward
	global PieceNumber
	global noPieceNumber
	global pieceDict
	x, y = getCoor(king)
	if direction == 0 and y != 0 and freeSquare(x, y-1):
		setCoor(king, x, y-1)
		return checkIsLegal(king, x,y)
	if direction == 1 and y != 0 and x != 0 and freeSquare(x-1, y-1):
		setCoor(king, x-1, y-1)
		return checkIsLegal(king, x, y)
	if direction == 2 and y != 0 and x != 7 and freeSquare(x+1, y-1):
		setCoor(king, x+1, y-1)
		return checkIsLegal(king, x, y)
	if direction == 3 and x != 0 and freeSquare(x-1, y):
		setCoor(king, x-1, y)
		return checkIsLegal(king, x, y)
	if direction == 4 and x != 7 and freeSquare(x+1, y):
		setCoor(king, x+1, y)
		return checkIsLegal(king, x, y)
	if direction == 5 and y != 7 and freeSquare(x, y+1):
		setCoor(king, x, y+1)
		return checkIsLegal(king, x, y)
	if direction == 6 and y != 7 and x != 0 and freeSquare(x-1, y+1):
		setCoor(king, x-1, y+1)
		return checkIsLegal(king, x, y)
	if direction == 7 and y != 7 and x != 7 and freeSquare(x+1, y+1):
		setCoor(king, x+1, y+1)
		return checkIsLegal(king, x, y)
	else:
		return False

# set coors but also back
def tryToMovePawn(direction):
	global board
	global piecesList
	if 'P' in piecesList:
		x, y = getCoor('P')
		if direction == 8 and y != 0 and freeSquare(x, y - 1):
			setCoor('P', x, y - 1)
			return checkIsLegal('P', x,y)
	else:
		return False

def movePawn(direction):
	global board
	global piecesList
	if 'P' in piecesList:
		x, y = getCoor('P')
		if direction == 8 and y != 0  and freeSquare(x, y-1):
			board[2][y-1][x] = PieceNumber
			board[2][y][x] = noPieceNumber
			setCoor('P', x, y-1)
			return True
	else:
		return False


# only for king and pawn
def takePiece():
	global noPieceNumber
	global piecesList
	if 'P' in piecesList:
		xk, yk = getCoor('k')
		xP, yP = getCoor('P')
		if xk == xP and yk == yP:
			board[2][y][x] = noPieceNumber
			piecesList.remove('P')
			print ' piece removed'
			#setCoor('P', -1, -1)

# # if piece is taken but illegal (work around)
# def redoTakePiece():
# 	global PieceNumber
# 	global piecesList
# 	x, y = getCoor('k')
# 	if enemyPiece(x, y) == True:
# 		board[2][y][x] = PieceNumber
# 		piecesList.append('P')

def movePieceIfPossible(direction):
	if direction <= 7:
		moveOnBoard = moveKing(direction, 'K')
		return moveOnBoard
	if direction == 8:
		moveOnBoard = movePawn(direction)
		return moveOnBoard


def tryToMovePiece(direction):
	if direction <= 7:
		return tryToMoveKing(direction, 'K')
	if direction == 8:
		return tryToMovePawn(direction)


# black king
def moveKingRandom():
	legalMove = False
	while legalMove == False:
		rand = random.randint(0,7)
		legalMove = moveKing(rand, 'k')

def boardTo2D():
	global board2D
	global board
	global piecesList
	board2D = np.chararray((8,8))
	board2D[:] = '.'
	pieceDict = {0 : 'K', 1 : 'k', 2: 'P'}
	print piecesList
	for piece in piecesList:
		x, y = getCoor(piece)
		board2D[y][x] = piece

def printBoard():
	global board2D
	global turn
	global halfmoves
	global reward
	boardTo2D()
	print board2D
	print ' '
	"""
	print 'turn = '
	print turn
	#print 'position is legal = '
	#print isLegal()
	print 'halfmoves = '
	print halfmoves
	print 'terminal state '
	print isTerminalState()
	print 'reward = '
	print reward
	"""

# for black king
def inCheck():
	global board
	global piecesList
	if 'P' in piecesList:
		xP, yP = getCoor('P')
		xk, yk = getCoor('k')
		if yk == yP -1 and (xk == xP - 1 or xk == xP + 1):
			return True
	else: 
		return False

# add pieces on top of each other
# if currentstate is illegal, if kings touch oneanother (or are on top of eachother,
# can only happen with setup, so re place later)
def isLegal():
	global side
	if side == True and inCheck():
		return False
	# only when more pieces are on board
	if len(board) > 1:
		xk, yk = getCoor('k')
		xK, yK = getCoor('K')
		if ((yk == yK -1 and (xk == xK - 1 or xk == xK or xk == xK + 1))
			or (yk == yK and (xk == xK - 1 or xk == xK or xk == xK + 1))
			or (yk == yK +1 and (xk == xK - 1 or xk == xK or xk == xK + 1))
			or (xk == xK and yk == yK)):
			return False
		else:
			return True
	else:
		return True

# if king's at the end of the board
def isTerminalState():
	global reward
	global piecesList
	if 'P' in piecesList:
		xK, yK = getCoor('K')
		#for pawn training
		xP, yP = getCoor('P')
		if yP == 0 and yK <= 1 and (xK == xP -1 or xK == xP + 1 or xK == xP):
			return 'won'
		elif stalemate():
			return 'drawn'
		else:
			return False
	else:
		return 'drawn'

# stalemate
def stalemate():
	global side
	kCanMove = False
	KCanMove = False
	for i in range(7):
		if tryToMoveKing(i,'k') == True:
			kCanMove = True
	for i in range(7):
		if tryToMoveKing(i, 'K') == True:
			KCanMove = True
	if tryToMovePawn(8) == True:
		KCanMove = True

	if kCanMove == False and side == False:
		return True
	if KCanMove == False and side == True:
		return True
	if kCanMove == True and KCanMove == True:
		return False

def boardSetupStalemate():
	global side
	boardInit(3)
	setPiece('P', 0,1)
	setPiece('k',2,0)
	setPiece('K', 0,0)
	side = False

print getCoor('k')
boardSetupStalemate()
printBoard()
print stalemate()
movePiece(5)
printBoard()
print stalemate()

# When creating random positions watch out for illegal positions

def boardSetupTraining():
	boardInit(2)
	setPieceRandom('K')
	if setPiece('k', 4, 2) == False:
		boardSetupTraining()
	if isLegal() == False:
		boardSetupTraining()

def boardSetupOneKing():
	boardInit(1)
	setPieceRandom('K')

def boardSetupPawn():
	global side
	boardInit(3)
	setPieceSemiRandom('P')
	#setPiece('P', 4,0)
	#setPiece('K',4,4)
	setPieceSemiRandom('K')
	setPieceSemiRandom('k')
	side = True

def boardSetupPawn2():
	boardInit(3)
	#setPieceSemiRandom('P')
	setPiece('P', 4,0)
	setPiece('k',4,1)
	setPieceSemiRandom('K')
	#setPieceSemiRandom('k')
	if isLegal() == False:
		boardSetupPawn2()

# like numpy reshape, returns flattend array of 2d board, with shape (1,64)
def flattenChessboard():
	global board
	amountPieces = len(board)
	newArray = np.zeros((amountPieces,64))
	#np.flatten()
	for piece in range(len(board)):
		for row in range(len(board[piece])):
			for number in range(len(board[piece][row])):
				newArray[piece][row*8+number] = board[piece][row][number]
	return newArray

"""
notes:

"""

# boardSetupPawn2()
#
# print movePiece(8)
# printBoard()
# print isTerminalState()
# print side
# print isLegal()
# print movePiece(0)
# printBoard()
#
# print side
# print isLegal()
#
# print movePiece(8)
# printBoard()
# print side
# print isLegal()
#
# print movePiece(3)
# printBoard()
# print side
# print isLegal()
#
# print movePiece(0)
# printBoard()
# print side
# print isLegal()
#
# print movePiece(5)
# printBoard()
# print side
# print isLegal()
#
# print board
# print getCoor('k')
# print getCoor('K')
# print getCoor('P')

# la = np.ones(192)
# print  np.expand_dims(la, 0)
# print np.array([la])
# print [la]


"""
NETWORK
"""
batchSize = 32

boardSetupPawn()
shapeBoardOneBatch = np.expand_dims(board.flatten(), 0).shape
shapeBoard = len(board.flatten())


X_sym = T.matrix()
Y_sym = T.matrix()
rew = T.ivector()
disc = T.vector()
action = T.ivector()
action_target = T.ivector()

# linear regression (?)
l_in = lasagne.layers.InputLayer(shape=(None, shapeBoard))
l_hidden = lasagne.layers.DenseLayer(l_in, num_units=100, nonlinearity = rectify)
l_hidden2 = lasagne.layers.DenseLayer(l_hidden, num_units=50, nonlinearity = rectify)
l_hidden3 = lasagne.layers.DenseLayer(l_hidden2, num_units=25, nonlinearity = rectify)
l_out = lasagne.layers.DenseLayer(l_hidden2, num_units=9, nonlinearity = None)
all_param_values = lasagne.layers.get_all_param_values(l_out)

# same but for target, copy weights from trainable network
l_in_target = lasagne.layers.InputLayer(shape=(None, shapeBoard))
l_hidden_target = lasagne.layers.DenseLayer(l_in_target, num_units=100, nonlinearity = rectify)
l_hidden2_target = lasagne.layers.DenseLayer(l_hidden_target, num_units=50, nonlinearity = rectify)
l_hidden3_target = lasagne.layers.DenseLayer(l_hidden2_target, num_units=25, nonlinearity = rectify)
l_out_target = lasagne.layers.DenseLayer(l_hidden2_target, num_units=9, nonlinearity = None)
lasagne.layers.set_all_param_values(l_out_target, all_param_values)

# trainable network
output = lasagne.layers.get_output(l_out, X_sym)
params = lasagne.layers.get_all_params(l_out)
actionBest = output.argmax(-1)
actionValueBest = output.max(-1)
actionValue = output[0][action]
actionValues = output[0]

# target network
output_target = lasagne.layers.get_output(l_out_target, Y_sym)
params_target = lasagne.layers.get_all_params(l_out_target)
actionBestTargetValue = output_target.max()
actionBestTarget = output_target.argmax(-1)

# general lasagne
#loss = lasagne.objectives.squared_error(disc*actionBestTargetValue + rew, actionValue)
#loss = T.mean((output[T.arange(batchSize), action.reshape((-1,))] - (rew + (disc * actionBestTargetValue)))**2)
target = (rew + disc * output_target[T.arange(batchSize), action_target])
network = output[T.arange(batchSize), action]
diff = target - network
loss = 0.5 * diff ** 2
loss = T.mean(loss)
grad = T.grad(loss, params)
updates = lasagne.updates.sgd(grad, params, learning_rate=0.01)

# theano functions
f_train = theano.function([X_sym, action, Y_sym, rew, disc, action_target], loss, updates=updates, allow_input_downcast=True)
f_predict = theano.function([X_sym], actionBest, allow_input_downcast=True)
f_predict_target_value = theano.function([Y_sym], actionBestTargetValue, allow_input_downcast=True)
q_val = theano.function([X_sym, action], actionValue, allow_input_downcast=True)
q_bestval = theano.function([X_sym], actionValueBest, allow_input_downcast=True)
q_vals = theano.function([X_sym], actionValues, allow_input_downcast=True)
f_predict_target = theano.function([Y_sym], actionBestTarget, allow_input_downcast=True)

# helper functions
grad_calc = theano.function([X_sym, action, Y_sym, rew, disc, action_target], grad)
output_calc = theano.function([X_sym], output)
network_calc = theano.function([X_sym, action], network)
target_calc = theano.function([Y_sym, rew, disc, action_target], target)

# epsilon-greedy, return the chosen move. high epsilon for random moves
def performAction(epsilon, state):
	randomNumber = random.uniform(0,1)
	moveLegal = True
	redoMoveDict = {0:5, 1:7, 2:6, 3:4, 4:3, 5:0, 6:2, 7:1}

	if randomNumber < epsilon:
		move = random.randint(0,8)
	else:
		move = f_predict(state)
		# to get rid of the array
		move = move[0]

	# performs move only if move is legal
	moveOnBoard = movePiece(move)
	if moveOnBoard == False:
		moveLegal = False
	if isLegal() == False:
		redoMove = redoMoveDict.get(move)
		movePiece(redoMove)
		moveLegal = False

	return move, moveLegal

replayMemory = []
epsilon = 0.8
iterationsToTrain = 0

# globals for evaluation while training
amountOfEpisodesTraining = 0
amountOfMovesTraining = 0
cumulatedLossTrain = 0
averageLossTraining = []

cumulatedLossEpisode = 0
cumulatedLossEpisodeList = []

bestQValues = []

# globals for evaluation in between training
amountOfEpisodesTest = 0
lengthEpisodeList = []
accumulatedLengthEpisode = 0
averageLengthEpisode = 0
averageLengthEpisodeList = []
amountOfInfiniteLoops = 0
amountOfInfiniteLoopsList = []

amountOfLoops = 0
timePerLoop = []

#iterationNumber = 0

# perform action and get reward
def fillRM():
	global all_param_valueslist
	global bestQValues
	global replayMemory
	global amountOfMovesTraining
	global epsilon

	global cumulatedLossEpisode

	amountOfMovesTraining += 1
	#epsilon = 1 - ((float(amountOfMovesTraining)-((1/10)*iterationsToTrain))/iterationsToTrain)
	if amountOfMovesTraining % 4000 == 0 and amountOfMovesTraining > 0:
		epsilon = max(0.1, epsilon - 0.1)

	state = board.flatten()
	move, moveLegal = performAction(epsilon, np.expand_dims(state, 0))
	stateHalf = np.expand_dims(board.flatten(), 0)
	# makes sure that both kings actually do a move
	if moveLegal == True:
		iets, moveLegalk = performAction(1, stateHalf) # the other king
		while moveLegalk == False:
			iets, moveLegalk = performAction(1, stateHalf)
	newState = board.flatten()

	if moveLegal == False and stalemate() == False:
		reward = -10
		discount = 0.99
	else:
		if isTerminalState() == 'won':
			reward = 10
			discount = 0
		if isTerminalState() == 'drawn':
			reward = -10
			discount = 0
		if isTerminalState() == False:
			reward = -1
			discount = 0.99

	# save experience in RM
	if len(replayMemory) < 1000:
		replayMemory.append([state, move, reward, newState, discount])
	else:
		replayMemory.pop(0)
		replayMemory.append([state, move, reward, newState, discount])

	#for evaluation
	bestQValue = q_bestval(np.expand_dims(state, 0))
	bestQValue = bestQValue[0]
	bestQValues.append(bestQValue)

	#loss = f_train(state, move, newState, reward, discount)
	#cumulatedLossEpisode += loss

	# print bestQValue
	# print output_calc(state)
	# all_param_values = lasagne.layers.get_all_param_values(l_out)
	# print np.dot(state, all_param_values[0]) + all_param_values[1]

# pick random states from RM and train
def trainOnRM():
	global replayMemory
	global cumulatedLossEpisode
	global batchSize

	if len(replayMemory) > 100:
		states = []
		moves = []
		rewards = []
		newStates = []
		discounts = []
		targetMoves = []
		for sample in range(batchSize):
			randomNumber = random.randint(0, len(replayMemory) - 1)
			states.append(replayMemory[randomNumber][0])
			moves.append(replayMemory[randomNumber][1])
			rewards.append(replayMemory[randomNumber][2])
			newStates.append(replayMemory[randomNumber][3])
			discounts.append(replayMemory[randomNumber][4])
			targetMove = f_predict_target(np.expand_dims(replayMemory[randomNumber][3],0))
			targetMove = targetMove[0]
			targetMoves.append(targetMove)
			# update params
		#print network_calc(states, moves)
		#print target_calc(newStates, rewards, discounts)
		#print discounts
		#print moves
		#print targetMoves
		loss = f_train(states, moves, newStates, rewards, discounts, targetMoves)
		cumulatedLossEpisode += loss

# the main training loop
def trainLoop(iterations):
	global replayMemory

	global amountOfEpisodesTraining
	global cumulatedLossTrain
	global averageLossTraining
	global cumulatedLossEpisode
	global cumulatedLossEpisodeList

	global bestQValues
	counter = 0

	for i in range(iterations):
		amountOfEpisodesTraining += 1
		cumulatedLossEpisode = 0

		# update target network
		counter += 1
		if counter == 50:
			all_param_values = lasagne.layers.get_all_param_values(l_out)
			lasagne.layers.set_all_param_values(l_out_target, all_param_values)
			counter = 0

		# to play games
		boardSetupPawn()
		while isTerminalState() == False:
			print epsilon
			fillRM()
			trainOnRM()
		print isTerminalState()
		# for evaluation
		cumulatedLossTrain += cumulatedLossEpisode
		cumulatedLossEpisodeList.append(cumulatedLossEpisode)
		averageLossTraining.append(cumulatedLossTrain/amountOfEpisodesTraining)

# to test
def test(iterations):
	amountOfInfiniteLoops = 0
	amountOfMoves = 0
	wins = 0
	draws = 0
	for i in range(iterations):
		boardSetupPawn()
		# printBoard()
		counter = 0
		lengthEpisode = 0
		while isTerminalState() == False and counter < 50:
			state = np.expand_dims(board.flatten(), 0)
			qVals = q_vals(state)
			print np.sort(qVals)
			print np.argsort(qVals, axis=0)
			printBoard()
			move, moveLegal = performAction(0, state)
			while moveLegal == False and stalemate() == False:
				move, moveLegal = performAction(1, state)
			stateHalf = np.expand_dims(board.flatten(), 0)
			# makes sure that k actually does a move
			if moveLegal == True:
				iets, moveLegalk = performAction(1, stateHalf)  # the other king
				while moveLegalk == False:
					iets, moveLegalk = performAction(1, stateHalf)

			print '-'*50
			if counter == 30:
				amountOfInfiniteLoops += 1
				print 'INFINITE LOOP!'
				print '-' * 50
				print '-' * 50
				print '-' * 50
				print '-' * 50
				print '-' * 50

			counter += 1
			amountOfMoves += 1
			lengthEpisode += 1
		printBoard()
		if isTerminalState() == 'won':
			wins += 1
		if isTerminalState() == 'drawn':
			draws += 1



		#printBoard()
		print lengthEpisode
	print '-' * 50
	print amountOfInfiniteLoops
	print amountOfMoves/(iterations)
	print wins
	print draws

# to evaluate during training
def evaluateDuringTraining(iterations):
	global amountOfEpisodesTest
	global lengthEpisodeList
	global accumulatedLengthEpisode
	global averageLengthEpisodeList
	global averageLengthEpisode
	global amountOfInfiniteLoops
	global amountOfInfiniteLoopsList

	for i in range(iterations):
		amountOfEpisodesTest += 1
		boardSetupPawn()
		counter = 0
		lengthEpisode = 0.0
		while isTerminalState() == False and counter < 50:
			state = np.expand_dims(board.flatten(), 0)
			performAction(0, state)
			counter += 1
			lengthEpisode += 1
			if counter == 49:
				amountOfInfiniteLoops += 1

		lengthEpisodeList.append(lengthEpisode)
		accumulatedLengthEpisode += lengthEpisode
		averageLengthEpisodeList.append(accumulatedLengthEpisode/amountOfEpisodesTest)
		amountOfInfiniteLoopsList.append(amountOfInfiniteLoops)

def evaluate():
	# while training
	global amountOfMovesTraining
	global amountOfEpisodesTraining
	global cumulatedLossEpisodeList
	global averageLossTraining
	# while evaluating during training
	global amountOfEpisodesTest
	global lengthEpisodeList
	global averageLengthEpisodeList
	global amountOfInfiniteLoops
	global amountOfInfiniteLoopsList

	global bestQValues
	global amountOfLoops
	global timePerLoop

	#evaluation
	# plot q vals of best move in all episodes
	amountOfEpisodesTrainingList = np.arange(amountOfEpisodesTraining)
	amountOfMovesTrainingList = np.arange(amountOfMovesTraining)

	plt.figure()
	plt.plot(amountOfMovesTrainingList, bestQValues)
	#plt.show()
	# plot cost per episode
	# plt.figure()
	# plt.plot(amountOfEpisodesTrainingList, cumulatedLossEpisodeList)
	plt.figure()
	plt.plot(amountOfEpisodesTrainingList[500:], averageLossTraining[500:])

	amountOfEpisodesTestList = np.arange(amountOfEpisodesTest)
	plt.figure()
	plt.plot(amountOfEpisodesTestList[30:], lengthEpisodeList[30:])
	plt.figure()
	plt.plot(amountOfEpisodesTestList, averageLengthEpisodeList)
	plt.figure()
	plt.plot(amountOfEpisodesTestList, amountOfInfiniteLoopsList)
	plt.figure()
	amountOfLoopsList = np.arange(amountOfLoops)
	plt.plot(amountOfLoopsList, timePerLoop)
	plt.show()

modelFile = 'Models/SavedModel.txt'
def saveModel():
	global modelFile
	all_param_values = lasagne.layers.get_all_param_values(l_out)
	with open(modelFile, 'w') as f:
		pickle.dump(all_param_values, f)

def loadModel():
	global modelFile
	with open(modelFile, 'r') as f:
		loadedParams = pickle.load(f)
	lasagne.layers.set_all_param_values(l_out, loadedParams)
	lasagne.layers.set_all_param_values(l_out_target, loadedParams)

outputFile1  = 'Models/SavedReplaymemory.npy'
outputFile2  = 'Models/amountOfMovesTraining.npy'
outputFile3  = 'Models/amountOfEpisodesTraining.npy'
outputFile4  = 'Models/cumulatedLossEpisodeList.npy'
outputFile5  = 'Models/averageLossTraining.npy'
outputFile6  = 'Models/amountOfEpisodesTest.npy'
outputFile7  = 'Models/lengthEpisodeList.npy'
outputFile8  = 'Models/averageLengthEpisodeList.npy'
outputFile9  = 'Models/bestQValues.npy'
outputFile10  = 'Models/epsilon.npy'

outputFile11 = 'Models/iterationsToTrain.npy'
outputFile12 = 'Models/cumulatedLossTrain.npy'
outputFile13 = 'Models/cumulatedLossEpisode.npy'
outputFile14 = 'Models/accumulatedLengthEpisode.npy'
outputFile15 = 'Models/averageLengthEpisode.npy'
outputFile16 = 'Models/amountOfInfiniteLoops.npy'
outputFile17 = 'Models/amountOfInfiniteLoopsList.npy'
outputFile18 = 'Models/amountOfLoops.npy'
outputFile19 = 'Models/timePerLoop.npy'

def saveEvaluation():
	global replayMemory

	global amountOfMovesTraining
	global amountOfEpisodesTraining
	global cumulatedLossEpisodeList
	global averageLossTraining
	global amountOfEpisodesTest
	global lengthEpisodeList
	global averageLengthEpisodeList
	global bestQValues

	global epsilon

	global iterationsToTrain
	global cumulatedLossTrain
	global cumulatedLossEpisode
	global accumulatedLengthEpisode
	global averageLengthEpisode
	global amountOfInfiniteLoops
	global amountOfInfiniteLoopsList
	global amountOfLoops
	global timePerLoop

	np.save(outputFile1, replayMemory)
	np.save(outputFile2, amountOfMovesTraining)
	np.save(outputFile3, amountOfEpisodesTraining)
	np.save(outputFile4, cumulatedLossEpisodeList)
	np.save(outputFile5, averageLossTraining)
	np.save(outputFile6, amountOfEpisodesTest)
	np.save(outputFile7, lengthEpisodeList)
	np.save(outputFile8, averageLengthEpisodeList)
	np.save(outputFile9, bestQValues)
	np.save(outputFile10, epsilon)
	np.save(outputFile11, iterationsToTrain)
	np.save(outputFile12, cumulatedLossTrain)
	np.save(outputFile13, cumulatedLossEpisode)
	np.save(outputFile14, accumulatedLengthEpisode)
	np.save(outputFile15, averageLengthEpisode)
	np.save(outputFile16, amountOfInfiniteLoops)
	np.save(outputFile17, amountOfInfiniteLoopsList)
	np.save(outputFile18, amountOfLoops)
	np.save(outputFile19, timePerLoop)


def loadEvaluation():
	global replayMemory

	global amountOfMovesTraining
	global amountOfEpisodesTraining
	global cumulatedLossEpisodeList
	global averageLossTraining
	global amountOfEpisodesTest
	global lengthEpisodeList
	global averageLengthEpisodeList
	global bestQValues

	global epsilon

	global iterationsToTrain
	global cumulatedLossTrain
	global cumulatedLossEpisode
	global accumulatedLengthEpisode
	global averageLengthEpisode
	global amountOfInfiniteLoops
	global amountOfInfiniteLoopsList
	global amountOfLoops
	global timePerLoop

	replayMemory = np.ndarray.tolist(np.load(outputFile1))
	amountOfMovesTraining = np.ndarray.tolist(np.load(outputFile2))
	amountOfEpisodesTraining = np.ndarray.tolist(np.load(outputFile3))
	cumulatedLossEpisodeList = np.ndarray.tolist(np.load(outputFile4))
	averageLossTraining = np.ndarray.tolist(np.load(outputFile5))
	amountOfEpisodesTest = np.ndarray.tolist(np.load(outputFile6))
	lengthEpisodeList = np.ndarray.tolist(np.load(outputFile7))
	averageLengthEpisodeList = np.ndarray.tolist(np.load(outputFile8))
	bestQValues = np.ndarray.tolist(np.load(outputFile9))
	epsilon = np.ndarray.tolist(np.load(outputFile10))
	iterationsToTrain = np.ndarray.tolist(np.load(outputFile11))
	cumulatedLossTrain = np.ndarray.tolist(np.load(outputFile12))
	cumulatedLossEpisode = np.ndarray.tolist(np.load(outputFile13))
	accumulatedLengthEpisode = np.ndarray.tolist(np.load(outputFile14))
	averageLengthEpisode = np.ndarray.tolist(np.load(outputFile15))
	amountOfInfiniteLoops = np.ndarray.tolist(np.load(outputFile16))
	amountOfInfiniteLoopsList = np.ndarray.tolist(np.load(outputFile17))
	amountOfLoops = np.ndarray.tolist(np.load(outputFile18))
	timePerLoop = np.ndarray.tolist(np.load(outputFile19))

def train():
	global amountOfLoops
	global timePerLoop
	#global iterationNumber
	#
	#loadModel()
	#loadEvaluation()
	amountOfLoopsJ = 20
	amountOfLoops += amountOfLoopsJ
	iterations = 102

	for i in range(amountOfLoopsJ):
		print i
		print epsilon
		t0 = time.clock()
		trainLoop(iterations)
		print time.clock() - t0, "seconds process time"
		timePerLoop.append(time.clock() - t0)
		print '-' * 50
		#evaluateDuringTraining(50)

	test(10)
	evaluate()
	saveModel()
	saveEvaluation()


train()


# sierk


# boardSetupPawn()
# state = np.expand_dims(board.flatten(), 0)
# #move, il = performAction(0,state)
# fillRM()
# fillRM()
# fillRM()
# print replayMemory




"""
average loss zou omlaag moeten gaan
"""
