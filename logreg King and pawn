import numpy as np
import random
import theano
import theano.tensor as T
import lasagne
from lasagne.layers import InputLayer, DenseLayer
from lasagne.nonlinearities import LeakyRectify

board = 0
board2D = 0
pieceDict = {'K' : 0, 'k' : 1, 'P' : 2}
pieceDict2 = {0 : 'K', 1 : 'k', 2: 'P'}
turn = True
halfmoves = 0
reward = 0
discount = 0.9

# for board representation
noPieceNumber = -1.0/64.0
PieceNumber = 63.0/64.0

# reset all board parameters
def boardInit(amountPieces):
	global board
	global turn
	global halfmoves
	global reward
	global noPieceNumber
	board = np.empty(64*amountPieces).reshape((amountPieces,8,8))
	for x in range(len(board)):
		for y in range(len(board[x])):
			board[x][y] = noPieceNumber
	turn = True
	halfmoves = 0
	reward = 0

# set piece, doesn't remove the old
def setPiece(piece, x, y):
	global board
	global pieceDict
	global pieceDict2
	global PieceNumber
	if freeSquare(x, y):
		board[pieceDict.get(piece)][y][x] = PieceNumber
		return True
	else:
		return False

def freeSquare(x, y):
	for piece in range(len(board)):
		pieceString = "'" + pieceDict2.get(piece) + "'"
		if getCoor(pieceDict2.get(piece)) != None:
			xx, yy = getCoor(pieceDict2.get(piece))
			if x == xx and y == yy:
				return False
	return True

def setPieceRandom(piece):
	onFreeSquare = False
	# checks whether square is free to add piece.
	while onFreeSquare == False:
		xRandom = random.randint(0,7)
		yRandom = random.randint(0,7)
		onFreeSquare = setPiece(piece, xRandom, yRandom)
"""
not used
def movePiece(piece, x, y):
	global board
	global pieceDict
	global turn
	global halfmoves
	global PieceNumber
	global noPieceNumber
	for n in range(x):
		for m in range(y):
			board[pieceDict.get(piece)][m][n] = noPieceNumber
	board[pieceDict.get(piece)][y][x] = PieceNumber
	halfmoves = halfmoves + 1
	if turn == True:
		turn = False
	else:
		turn = True
"""
# moves whiteKing into direction
def moveKing(direction):
	global board
	global reward
	global PieceNumber
	global noPieceNumber
	x, y = getCoor('K')
	if direction == 0 and y != 0 and freeSquare(x, y-1):
		board[0][y-1][x] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	if direction == 1 and y != 0 and x != 0 and freeSquare(x-1, y-1):
		board[0][y-1][x-1] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	if direction == 2 and y != 0 and x != 7 and freeSquare(x+1, y-1):
		board[0][y-1][x+1] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	if direction == 3 and x != 0 and freeSquare(x-1, y):
		board[0][y][x-1] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	if direction == 4 and x != 7 and freeSquare(x+1, y):
		board[0][y][x+1] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	if direction == 5 and y != 7 and freeSquare(x, y+1):
		board[0][y+1][x] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	if direction == 6 and y != 7 and x != 0 and freeSquare(x-1, y+1):
		board[0][y+1][x-1] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	if direction == 7 and y != 7 and x != 7 and freeSquare(x+1, y+1):
		board[0][y+1][x+1] = PieceNumber
		board[0][y][x] = noPieceNumber
		return True
	else: 
		return False

def movePawn(direction):
	global board
	x, y = getCoor('P')
	if direction == 8 and y != 0  and freeSquare(x, y-1):
		board[2][y-1][x] = PieceNumber
		board[2][y][x] = noPieceNumber
		return True
	else:
		return False

def movePiece(direction):
	if direction <= 7:
		moveOnBoard = moveKing(direction)
		return moveOnBoard
	if direction == 8:
		moveOnBoard = movePawn(direction)
		return moveOnBoard

# whiteking
def moveKingRandom():
	rand = random.randint(0,7)
	moveKing(rand)
	return rand

def boardTo2D():
	global board2D
	global board
	board2D = np.chararray((8,8))
	board2D[:] = '.'
	pieceDict = {0 : 'K', 1 : 'k', 2: 'P'}
	for piece in range(len(board)):
		x, y = getCoor(pieceDict.get(piece))
		board2D[y][x] = pieceDict.get(piece)

def printBoard():
	global board2D
	global turn
	global halfmoves
	global reward
	boardTo2D()
	print board2D
	print ' '
	"""
	print 'turn = '
	print turn
	#print 'position is legal = '
	#print isLegal()
	print 'halfmoves = '
	print halfmoves
	print 'terminal state '
	print isTerminalState()
	print 'reward = '
	print reward
	"""

def getCoor(piece):
	global board
	piece = pieceDict.get(piece)
	for y in range(len(board[piece])):
		for x in range(len(board[piece][y])):
			if board[piece][y][x] > 0:
				return x, y

# for black king
def inCheck():
	global board
	xP, yP = getCoor('P')
	xk, yk = getCoor('k')
	if yk == yP -1 and (xk == xP - 1 or xk == xP + 1):
		return True
	else: 
		return False

# add pieces on top of each other
# if currentstate is legal, if kings touch oneanother (or are on top of eachother, 
# can only happen with setup, so re place later)
def isLegal():
	global turn
	# if turn == True and inCheck():
	# 	return False
	if len(board) > 1:
		xk, yk = getCoor('k')
		xK, yK = getCoor('K')
		if ((yk == yK -1 and (xk == xK - 1 or xk == xK or xk == xK + 1))
			or (yk == yK and (xk == xK - 1 or xk == xK or xk == xK + 1))
			or (yk == yK +1 and (xk == xK - 1 or xk == xK or xk == xK + 1))
			or (xk == xK and yk == yK)):
			return False
	else:
		return True

# if king's at the end of the board
def isTerminalState():
	global reward
	xK, yK = getCoor('K')
	xP, yP = getCoor('P')
	if yP == 0 and yK <= 1 and (xK == xP -1 or xK == xP + 1 or xK == xP):
		return True
	else:
		return False

def boardSetupTraining():
	boardInit(2)
	setPieceRandom('K')
	setPiece('k', 4, 2)
	if isLegal() == False:
		boardSetupTraining()

def boardSetupOneKing():
	boardInit(1)
	setPieceRandom('K')

def boardSetupPawn():
	boardInit(3)
	setPieceRandom('K')
	setPieceRandom('P')
	if setPiece('k', 7, 7) == False:
		boardSetupPawn()
	if isLegal() == False:
		boardSetupPawn()

def boardSetupTest():
	boardInit(2)
	setPiece('K', 4, 6)
	setPiece('k', 4, 2)
	if isLegal() == False:
		boardSetupTest()

# like numpy reshape, returns flattend array of 2d board, with shape (1,64)
def flattenChessboard():
	global board
	amountPieces = len(board)
	newArray = np.zeros((amountPieces,64))
	for piece in range(len(board)):
		for row in range(len(board[piece])):
			for number in range(len(board[piece][row])):
				newArray[piece][row*8+number] = board[piece][row][number]
	return newArray

"""
notes:

"""

"""
NETWORK
"""

# linear regression (?)
l_in = lasagne.layers.InputLayer((2, 64))
custom_rectify = LeakyRectify(1)
l_out = lasagne.layers.DenseLayer(l_in, num_units=9, nonlinearity = custom_rectify)
all_param_values = lasagne.layers.get_all_param_values(l_out)

# same but for target, copy weights from trainable network
l_in_target = lasagne.layers.InputLayer((2, 64))
l_out_target = lasagne.layers.DenseLayer(l_in_target, num_units=9, nonlinearity = custom_rectify)
lasagne.layers.set_all_param_values(l_out_target, all_param_values)


X_sym = T.matrix()
Y_sym = T.matrix()
rew = T.scalar()
disc = T.scalar()
action = T.iscalar()

# trainable network
output = lasagne.layers.get_output(l_out, X_sym)
params = lasagne.layers.get_all_params(l_out)
actionBest = output.argmax()
actionValue = output[0][action]
actionValues = output[0]

# target network
output_target = lasagne.layers.get_output(l_out_target, Y_sym)
params_target = lasagne.layers.get_all_params(l_out_target)
actionBestTargetValue = output_target.max()

# general lasagne
loss = lasagne.objectives.squared_error(disc*actionBestTargetValue + rew, actionValue)
grad = T.grad(loss, params)
updates = lasagne.updates.sgd(grad, params, learning_rate=0.05)

# theano functions
f_train = theano.function([X_sym, action, Y_sym, rew, disc], loss, updates=updates, allow_input_downcast=True)
f_predict = theano.function([X_sym], actionBest, allow_input_downcast=True)
f_predict_target_value = theano.function([Y_sym], actionBestTargetValue, allow_input_downcast=True)
q_val = theano.function([X_sym, action], actionValue)
q_vals = theano.function([X_sym], actionValues)

# helper functions
grad_calc = theano.function([X_sym, action, Y_sym, rew, disc], grad)
output_calc = theano.function([X_sym], output)

# epsilon-greedy, return the chosen move. high epsilon for random moves
def performAction(epsilon, state):
	randomNumber = random.uniform(0,1)
	moveOnBoard = False
	moveLegal = False
	redoMoveDict = {0:5, 1:7, 2:6, 3:4, 4:3, 5:0, 6:2, 7:1}
	if randomNumber < epsilon:
		randMove = 0
		while moveOnBoard == False or moveLegal == False:
			randMove = random.randint(0,8)
			# performs move if move is on board
			moveOnBoard = movePiece(randMove)
			# redoes move when position is illegal
			if isLegal() == False:
				redoMove = redoMoveDict.get(randMove)
				movePiece(redoMove)
			else:
				moveLegal = True
		return randMove
	else:
		qVals = q_vals(state)
		sortedMoves = np.argsort(-qVals, axis=0)
		counter = 0
		predictedMove = 0
		# choses next best move after illegal move
		while moveOnBoard == False or moveLegal == False:
			predictedMove = sortedMoves[counter]
			moveOnBoard = movePiece(predictedMove)
			counter += 1
			if isLegal() == False:
				redoMove = redoMoveDict.get(predictedMove)
				movePiece(redoMove)
			else:
				moveLegal = True
		return predictedMove

# the main training loop
def trainLoop(iterations):
	average_loss = 0
	reward = 0
	discount = 1
	counter = 0
	amountOfMoves = 0
	for i in range(iterations):
		lengthEpisode = 0
		boardSetupPawn()
		#printBoard()
		while isTerminalState() == False:
			state = flattenChessboard()
			x, y = getCoor('K')
			# epsilon decreases over time, -(i*(1/2*iterations))
			epsilon = 1 - ((float(i)-((1/10)*iterations))/iterations)
			move = performAction(epsilon, state)
			xNew, yNew = getCoor('K')
			if y > yNew:
				reward = float(y - yNew)
			if y <= yNew:
				reward = 0
			newState = flattenChessboard()
			if isTerminalState():
				loss = f_train(state, move, newState, 10, 0)
			else:
				loss = f_train(state, move, newState, -1, discount)
			amountOfMoves += 1
			lengthEpisode += 1

		# qVals = q_vals(state)
		# print np.sort(qVals)
		# print np.argsort(qVals, axis=0)
		# print f_predict_target_value(state)
		#doesn't work yet
		counter += 1
		if counter > 100:
			all_param_values = lasagne.layers.get_all_param_values(l_out)
			lasagne.layers.set_all_param_values(l_out_target, all_param_values)
			print ' updated'
			# state will always be terminal
			# qVals = q_vals(state)
			# print np.sort(qVals)
			# print np.argsort(qVals, axis=0)
			counter = 0

		#print lengthEpisode
		#print amountOfMoves/(i+1)

		#q values and gradient test
		# for i in range(8):
		# 	gradcalcs = grad_calc(predictedMoveTarget, i, state, reward, discount)
		# 	a,b = gradcalcs

		# 	print '-'*80
		# 	print np.sum(a**2)
		# 	print np.sum(b**2)
		# 	print q_val(state, i)
		#print average_reward
		#print average_loss/i	
	#gradcalcs = grad_calc(predictedMoveTarget, move, state, reward, discount)
	#print gradcalcs	

# weights = l_out.W.get_value()
# print weights

trainLoop(5000)

# weights2 = l_out.W.get_value()
# print weights2
# print weights2 - weights


# print weights
# print state
# print output_calc(state)
# print np.dot(state, weights)

#weights2 = l_out.W.get_value()
#print weights
#print weights2
#print weights-weights2

# to test
def test(iterations):
	amountOfMoves = 0
	for i in range(iterations):
		boardSetupPawn()
		# printBoard()
		counter = 0
		lengthEpisode = 0
		while isTerminalState() == False and counter < 30:
			state = flattenChessboard()

			qVals = q_vals(state)
			print np.sort(qVals)
			print np.argsort(qVals, axis=0)
			printBoard()
			performAction(0, state)

			counter += 1
			amountOfMoves += 1
			lengthEpisode += 1
			#print '-'*50
		printBoard()
		print lengthEpisode
	print amountOfMoves/(iterations)
		#print '-'*100

test(5)

"""
average loss zou omlaag moeten gaan
"""
